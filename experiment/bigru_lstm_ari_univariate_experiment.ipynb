{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_36qHcKgHBl"
      },
      "outputs": [],
      "source": [
        "!pip install scipy\n",
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOmnJ1hjkKc4"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.style.available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSOwuJ0TkLk0"
      },
      "outputs": [],
      "source": [
        "plt.style.use('seaborn-v0_8-white')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "data_path = '/content/drive/MyDrive/ESTUDIO_TIME_SERIES'\n",
        "\n",
        "# Local path\n",
        "#data_path = 'C:/Users/XXX/ESTUDIO_TIME_SERIES'"
      ],
      "metadata": {
        "id": "vnwUHIfbGrTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Es6L4nEckb3S"
      },
      "source": [
        "# 1. Dataset Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvROsCTA8nSB"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Reshape, Flatten, GRU\n",
        "from tensorflow.keras.optimizers import RMSprop, Adam, SGD\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "\n",
        "# df1 = pd.read_csv('cpu_percent_results_2.csv', usecols=['beginTimeSeconds', 'CPU percent'])\n",
        "# df2 = pd.read_csv('uptime_results_2.csv', usecols=['Uptime'])\n",
        "# df3 = pd.read_csv('tx_rx_results_2.csv',\n",
        "#                   usecols=['Rx packets', 'Tx packets'])\n",
        "# df4 = pd.read_csv('nb_np_results_2.csv',\n",
        "#                   usecols=['NetBytes In', 'NetBytes Out', 'NetPackets In', 'NetPackets Out'])\n",
        "# df5 = pd.read_csv('disk_memory_results_2.csv',\n",
        "#                   usecols=['Disk read/s', 'Disk write/s', 'Free Disk', 'Free Memory', 'Used Disk',\n",
        "#                            'Used Memory'], dtype='str')\n",
        "# df6 = pd.read_csv('disk_percent_results_2.csv', usecols=['Disk Used percent'])\n",
        "# df7 = pd.read_csv('memory_percent_results_2.csv', usecols=['Memory Used percent'])\n",
        "\n",
        "# df = pd.concat([df1, df2, df3, df4, df5, df6, df7], axis=1)\n",
        "# df = df2.copy()\n",
        "# df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLX9OtOsCqq2"
      },
      "outputs": [],
      "source": [
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPqg17Gyf3sR"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('data.csv', index_col=0, parse_dates=['beginTimeSeconds'])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8_FfUVXrYGN"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14xjAhSHn0SL"
      },
      "outputs": [],
      "source": [
        "new_order_columns = [\n",
        "    'Free Memory',\n",
        "    'Used Memory',\n",
        "    'Free Disk',\n",
        "    'Used Disk',\n",
        "    'Disk read/s',\n",
        "    'Disk write/s',\n",
        "    'NetBytes In',\n",
        "    'NetBytes Out',\n",
        "    'NetPackets In',\n",
        "    'NetPackets Out',\n",
        "    'Rx packets',\n",
        "    'Tx packets',\n",
        "    'CPU percent',\n",
        "    'Memory Used percent',\n",
        "    'Disk Used percent',\n",
        "    'Uptime',\n",
        "]\n",
        "df = df[new_order_columns]\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjoVojl-KC0x"
      },
      "outputs": [],
      "source": [
        "def cut_col_name(col_name):\n",
        "  return re.sub(\"[^a-zA-Z]\", '', col_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgV5krdHTUBP"
      },
      "outputs": [],
      "source": [
        "## df['beginTimeSeconds'] = pd.to_datetime(df['beginTimeSeconds'], unit='s')\n",
        "## df = df.set_index('beginTimeSeconds')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DLgLxpHrZVu"
      },
      "outputs": [],
      "source": [
        "# cols = df.columns\n",
        "\n",
        "# N = df.shape[0]\n",
        "# plots = df[['Free Disk', 'Used Disk', 'Disk Used percent']][:N]\n",
        "# # plots.index = df['beginTimeSeconds'][0:N]\n",
        "# plots.index = df.index[:N]\n",
        "# _ = plots.plot(subplots=True, figsize=(25,25))\n",
        "# plt.legend(fontsize='xx-large')\n",
        "\n",
        "# # plt.grid(True)\n",
        "# plt.savefig('dataset.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxU52hINDnxf"
      },
      "outputs": [],
      "source": [
        "fsize_labels = 23\n",
        "fsize_ticks = 25\n",
        "fsize_title = 23\n",
        "figsize = (15, 5)\n",
        "\n",
        "# for col in df.columns:\n",
        "\n",
        "#   fig, ax = plt.subplots(figsize = figsize)\n",
        "#   ax.plot(df.index, df[col].values)\n",
        "#   ax.set_title(col, fontsize=fsize_title)\n",
        "#   ax.set_xlabel('Time', fontsize=fsize_labels)\n",
        "#   ax.set_ylabel('Value', fontsize=fsize_labels)\n",
        "#   ax.tick_params(labelsize=fsize_ticks)\n",
        "#   ax.grid(False)\n",
        "\n",
        "#   plt.tight_layout()\n",
        "#   plt.savefig(f'{data_path}/DATA_COMPLETA/{cut_col_name(col)}_data.jpg')\n",
        "\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtiIzQmUTRHB"
      },
      "source": [
        "## 1.1. Feature management\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_05XHDNfdKQ"
      },
      "source": [
        "It is necessary to eliminate a certain segment of the information due to its lack of temporality, since it would generate errors during training.\n",
        "The information to be predicted corresponds to data collected after July 5, 2023."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cy3FP4DoCsth"
      },
      "outputs": [],
      "source": [
        "df = df[(df.index <= '2023-07-05 00:00:00')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFBD45qSqV7M"
      },
      "outputs": [],
      "source": [
        "# cols = df.columns\n",
        "\n",
        "# N = df.shape[0]\n",
        "# plots = df[cols[-4:]][:N]\n",
        "# # plots.index = df['beginTimeSeconds'][0:N]\n",
        "# plots.index = df.index[0:N] # Time Variable\n",
        "# _ = plots.plot(subplots=True, figsize=(25,25))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCQ7bbFgIhbF"
      },
      "outputs": [],
      "source": [
        "fsize_labels = 23\n",
        "fsize_ticks = 20\n",
        "fsize_title = 23\n",
        "\n",
        "# for col in df.columns:\n",
        "\n",
        "#   fig, ax = plt.subplots(figsize = figsize)\n",
        "#   ax.plot(df.index, df[col].values)\n",
        "#   ax.set_title(col, fontsize=fsize_title)\n",
        "#   ax.set_xlabel('Time', fontsize=fsize_labels)\n",
        "#   ax.set_ylabel('Value', fontsize=fsize_labels)\n",
        "#   ax.tick_params(labelsize=fsize_ticks)\n",
        "#   ax.grid(False)\n",
        "\n",
        "#   plt.tight_layout()\n",
        "#   plt.savefig(f'{data_path}/DATA_LIMITADA/{cut_col_name(col)}_data.jpg')\n",
        "\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMdgd4FLf9TJ"
      },
      "source": [
        "## 1.2. Descriptive statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmgVZfrwcK7Q"
      },
      "outputs": [],
      "source": [
        "for column in df.columns:\n",
        "  print(f'\\nVariable [{column}]\\n', df[column].describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5WcY7-5O1Lz"
      },
      "outputs": [],
      "source": [
        "for col in df.columns:\n",
        "  fig, axe = plt.subplots(figsize=(5, 7))\n",
        "  box = axe.boxplot(df[col], notch=True, vert=True, autorange=True, sym=\"\")\n",
        "  for box in box['boxes']:\n",
        "      box.set_color('b')\n",
        "      axe.set_title(col, fontsize=25)\n",
        "      axe.tick_params(labelsize=25)\n",
        "      plt.tight_layout()\n",
        "      # plt.savefig(f'{data_path}/DATA_STATISTICS/{cut_col_name(col)}_data.jpg')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNCpbNrGhHPC"
      },
      "source": [
        "## 1.3. Variability determination\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kL7_CfqoVTMq"
      },
      "outputs": [],
      "source": [
        "print(len(df.columns))\n",
        "for column in df.columns:\n",
        "  arr = df[column].unique()\n",
        "  print(f'Column: [{column}] :: Unique data [{arr}], Total Unique Data [{len(arr)}]')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJ7c6mdfVCfF"
      },
      "source": [
        "### 1.3.1 Correlation matrix\n",
        "\n",
        "The correlation of variables is identified.\n",
        "A higher correlation when the values are closer to -1 and 1.\n",
        "It indicates if the variables have an impact on each other."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "robgJnphdgDB",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "correlation_matrix = df.corr()\n",
        "\n",
        "print(correlation_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YiunafoJuHU9"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(25, 10))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='Blues', fmt=\".2f\", linewidths=.5)\n",
        "plt.title('Correlation Matrix')\n",
        "plt.savefig(f'/content/drive/MyDrive/ESTUDIO_TIME_SERIES/DATA_STATISTICS/cor_matrix.pdf')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vd5lyqNrlfFg"
      },
      "source": [
        "## 1.4. Data Pre-processing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hliQnKgIdfkF"
      },
      "source": [
        "### 1.4.1. Partitioning in Training, Validation and Testing datasets\n",
        "\n",
        "Unlike other Deep Learning models, in the case of time series **it must be guaranteed that the partitions are generated without randomly mixing the data**.\n",
        "\n",
        "Note:\n",
        "- The training set (*train*) will be used to find the model parameters.\n",
        "- The validation set (*val*) to verify that there is no *under/over-fitting* of the model and to adjust its hyperparameters.\n",
        "- The test set (*test*) to test the best model found during training/validation.\n",
        "\n",
        "In this case we will use the same function implemented for the univariate models with the difference that instead of entering a *series* of Pandas, we will enter the complete *DataFrame*.\n",
        "\n",
        "Therefore, the function will return three *dataframes* (train, val and test):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKWILYqZ-rBV"
      },
      "outputs": [],
      "source": [
        "fsize_labels = 23\n",
        "fsize_ticks = 16\n",
        "fsize_title = 25\n",
        "figsize = (15, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEO-j_e5nMPa"
      },
      "outputs": [],
      "source": [
        "datasets = {}\n",
        "# tr_ptg = 0.7\n",
        "# vl_ptg = 0.15\n",
        "# ts_ptg = 0.15\n",
        "num_folds = 4\n",
        "cut_dates = {}\n",
        "\n",
        "for i, fold in enumerate(range(0, num_folds - 1)):\n",
        "  len_data = len(df) / num_folds\n",
        "  len_train_data = int(len_data * (i + 1))\n",
        "\n",
        "  train_data = int(len_train_data * 0.8)\n",
        "  val_data = int(len_train_data * 0.2)\n",
        "\n",
        "  print('--' * 20)\n",
        "  print(f'Training - iteration {i}  has {len_train_data}')\n",
        "  tr = df[:train_data]\n",
        "  vl = df[train_data:train_data + val_data]\n",
        "  ts = df[len_train_data:len_train_data + int(len_data)]\n",
        "\n",
        "  cut_dates[i] = [tr.index[-1], vl.index[-1], ts.index[-1]]\n",
        "\n",
        "  print(f'Cut date train dataset: {tr.index[0]} - {tr.index[-1]}')\n",
        "  print(f'Cut date val dataset: {vl.index[0]} - {vl.index[-1]}')\n",
        "  print(f'Cut date test dataset: {ts.index[0]} - {ts.index[-1]}')\n",
        "  print('--' * 20)\n",
        "\n",
        "  datasets[i] = [tr, vl, ts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLmL2l3426HY"
      },
      "outputs": [],
      "source": [
        "line_width = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBB77LolWC0s"
      },
      "outputs": [],
      "source": [
        "n_fold = 3\n",
        "\n",
        "# for col in df.columns:\n",
        "#   for fold in range(n_fold):\n",
        "\n",
        "#     print(f'Presentando el fold {fold} para {col}')\n",
        "#     tr_ds = datasets[fold][0]\n",
        "#     val_ds = datasets[fold][1]\n",
        "#     ts_ds = datasets[fold][2]\n",
        "\n",
        "#     fig, ax = plt.subplots(figsize = figsize)\n",
        "\n",
        "#     ax.plot(tr_ds.index, tr_ds[col].values, linewidth=2)\n",
        "#     ax.plot(val_ds.index, val_ds[col].values, linewidth=2)\n",
        "#     ax.plot(ts_ds.index, ts_ds[col].values, linewidth=2)\n",
        "\n",
        "#     ax.set_title(f'Fold {fold} - {col}', fontsize=fsize_title)\n",
        "#     ax.set_xlabel('Time', fontsize=fsize_labels)\n",
        "#     ax.set_ylabel('Value', fontsize=fsize_labels)\n",
        "#     ax.tick_params(labelsize=fsize_ticks)\n",
        "#     ax.grid(False)\n",
        "#     plt.tight_layout()\n",
        "\n",
        "#     plt.savefig(f'{data_path}/SUPERVISED_DATASETS/{fold}_{cut_col_name(col)}_data.jpg')\n",
        "\n",
        "#     plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSq2AIcdxQ4n"
      },
      "outputs": [],
      "source": [
        "def create_supervised_dataset(dataset, input_length, output_length):\n",
        "  X = []\n",
        "  Y = []\n",
        "  rows = dataset.shape[0]\n",
        "  for i in range(rows - input_length - output_length + 1):\n",
        "    X.append(dataset[i:i + input_length])\n",
        "    Y.append(dataset[i + input_length:i + input_length + output_length])\n",
        "  return np.array(X), np.array(Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnHnsoUlHHF6"
      },
      "outputs": [],
      "source": [
        "tr_datasets = {}\n",
        "vl_datasets = {}\n",
        "ts_datasets = {}\n",
        "\n",
        "INPUT_LENGTH = 1\n",
        "OUTPUT_LENGTH = 1\n",
        "\n",
        "for fold, dataset in datasets.items():\n",
        "    tr, vl, ts = dataset\n",
        "    print('--' * 20)\n",
        "    print(f'Iteration {fold}: {tr.shape}, {vl.shape}, {ts.shape}')\n",
        "    print('--' * 20)\n",
        "\n",
        "    tr_datasets[fold] = create_supervised_dataset(tr.values, INPUT_LENGTH, OUTPUT_LENGTH)\n",
        "    vl_datasets[fold] = create_supervised_dataset(vl.values, INPUT_LENGTH, OUTPUT_LENGTH)\n",
        "    ts_datasets[fold] = create_supervised_dataset(ts.values, INPUT_LENGTH, OUTPUT_LENGTH)\n",
        "\n",
        "    print('Input (BATCHES x INPUT_LENGTH x FEATURES) and output sizes (BATCHES x OUTPUT_LENGTH x FEATURES)')\n",
        "    print(f'Train Set - x_tr: {tr_datasets[fold][0].shape}, y_tr: {tr_datasets[fold][1].shape}')\n",
        "    print(f'Validation Set - x_vl: {vl_datasets[fold][0].shape}, y_vl: {vl_datasets[fold][1].shape}')\n",
        "    print(f'Test Set - x_ts: {ts_datasets[fold][0].shape}, y_ts: {ts_datasets[fold][1].shape}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13SdoFggi_7Z"
      },
      "outputs": [],
      "source": [
        "def escalar_dataset(data_input):\n",
        "    NFEATS = data_input['x_tr'].shape[2]\n",
        "\n",
        "    # Generate list with \"scalers\" (1 for each input covariate)\n",
        "    scalers = [MinMaxScaler(feature_range=(-1,1)) for i in range(NFEATS)]\n",
        "    print(len(scalers), NFEATS)\n",
        "\n",
        "    # Arreglos que contendrán los datasets escalados\n",
        "    x_tr_s = np.zeros(data_input['x_tr'].shape)\n",
        "    x_vl_s = np.zeros(data_input['x_vl'].shape)\n",
        "    x_ts_s = np.zeros(data_input['x_ts'].shape)\n",
        "    y_tr_s = np.zeros(data_input['y_tr'].shape)\n",
        "    y_vl_s = np.zeros(data_input['y_vl'].shape)\n",
        "    y_ts_s = np.zeros(data_input['y_ts'].shape)\n",
        "\n",
        "    x_tr_ds, y_tr_ds = data_input['x_tr'], data_input['y_tr']\n",
        "    x_val_ds, y_val_ds = data_input['x_vl'], data_input['y_vl']\n",
        "    x_ts_ds, y_ts_ds = data_input['x_ts'], data_input['y_ts']\n",
        "\n",
        "    # Scaling: the min/max of the training set will be used to scale all the datasets.\n",
        "\n",
        "    # Scaling Xs\n",
        "    for i in range(NFEATS):\n",
        "        x_tr_s[:,:,i] = scalers[i].fit_transform(x_tr_ds[:,:,i])\n",
        "        x_ts_s[:,:,i] = scalers[i].transform(x_ts_ds[:,:,i])\n",
        "        x_vl_s[:,:,i] = scalers[i].transform(x_val_ds[:,:,i])\n",
        "\n",
        "        y_tr_s[:,:,i] = scalers[i].transform(y_tr_ds[:,:,i])\n",
        "        y_vl_s[:,:,i] = scalers[i].transform(y_val_ds[:,:,i])\n",
        "        y_ts_s[:,:,i] = scalers[i].transform(y_ts_ds[:,:,i])\n",
        "\n",
        "\n",
        "    # Create output dictionary\n",
        "    data_scaled = {\n",
        "        'x_tr_s': x_tr_s, 'y_tr_s': y_tr_s,\n",
        "        'x_vl_s': x_vl_s, 'y_vl_s': y_vl_s,\n",
        "        'x_ts_s': x_ts_s, 'y_ts_s': y_ts_s,\n",
        "    }\n",
        "\n",
        "    return data_scaled, scalers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9G-T7PdsjAlR"
      },
      "outputs": [],
      "source": [
        "tr_s_ds, vl_s_ds, ts_s_ds = {}, {}, {}\n",
        "scalers = {}\n",
        "\n",
        "index = 0\n",
        "\n",
        "for (x_tr_ds, y_tr_ds), (x_val_ds, y_val_ds), (x_ts_ds, y_ts_ds) in zip(tr_datasets.values(), vl_datasets.values(), ts_datasets.values()):\n",
        "\n",
        "    print('--' * 20)\n",
        "    print(f'Iteration {index}: {x_tr_ds.shape}, {x_val_ds.shape}, {x_ts_ds.shape}')\n",
        "    print('--' * 20)\n",
        "\n",
        "    data_in = {\n",
        "        'x_tr': x_tr_ds, 'y_tr': y_tr_ds,\n",
        "        'x_vl': x_val_ds, 'y_vl': y_val_ds,\n",
        "        'x_ts': x_ts_ds, 'y_ts': y_ts_ds,\n",
        "    }\n",
        "\n",
        "    # Y scalar (specifying the column with the variable to predict)\n",
        "    data_s, scalers_result = escalar_dataset(data_in,)\n",
        "\n",
        "    tr_s_ds[index] = data_s['x_tr_s'], data_s['y_tr_s']\n",
        "    vl_s_ds[index] = data_s['x_vl_s'], data_s['y_vl_s']\n",
        "    ts_s_ds[index] = data_s['x_ts_s'], data_s['y_ts_s']\n",
        "    scalers[index] = scalers_result\n",
        "\n",
        "    index += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fsR6fsbldup"
      },
      "outputs": [],
      "source": [
        "# And let's generate a violin plot to see the distribution of values in each covariate (input) and in the variable to predict (output).\n",
        "# index = 0\n",
        "# for (x_tr_s, y_tr_s), (x_vl_s, y_vl_s), (x_ts_s, y_ts_s) in zip(tr_s_ds.values(), vl_s_ds.values(), ts_s_ds.values()):\n",
        "#   print('--' * 20)\n",
        "#   print(f'Fold {index}: {x_tr_s.shape}, {x_vl_s.shape}, {x_ts_s.shape}')\n",
        "#   print('--' * 20)\n",
        "#   index += 1\n",
        "#   for i, col in enumerate(df.columns):\n",
        "#     plt.figure(figsize=(10, 8))\n",
        "\n",
        "#     plt.subplot(1, 1, 1)\n",
        "#     plt.violinplot(dataset=x_tr_s[:,:,i].flatten(), positions=[0])\n",
        "#     plt.violinplot(dataset=x_vl_s[:,:,i].flatten(), positions=[0])\n",
        "#     plt.violinplot(dataset=x_ts_s[:,:,i].flatten(), positions=[0])\n",
        "#     plt.title(f'Fold {index} - {col}', fontdict={'fontsize': 20})\n",
        "\n",
        "#     plt.xticks(fontsize=20)\n",
        "#     plt.yticks(fontsize=20)\n",
        "\n",
        "#     plt.grid(False)\n",
        "#     plt.savefig(f'{data_path}/DATA_STATISTICS/fold-{index}_{cut_col_name(col)}_violin.pdf')\n",
        "#     # plt.savefig(f'{col.replace(\"/\", \"_\")}_frecuencia.pdf')\n",
        "#     plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYmEzZiI_Kqc"
      },
      "source": [
        "# 2. BI-GRU Model Building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54w3pz9AYafx"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.metrics import RootMeanSquaredError, MeanAbsoluteError, MeanAbsolutePercentageError, MeanSquaredError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmK8SOgL9340"
      },
      "outputs": [],
      "source": [
        "dict_variables_results = {}\n",
        "\n",
        "for i, col in enumerate(df.columns):\n",
        "  dict_variables_results[col] = {\n",
        "      \"rmse\": 0,\n",
        "      \"mae\": 0,\n",
        "      \"mape\": 0,\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YHawTqM8rGM"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Bidirectional, Flatten, Conv1D, Dropout\n",
        "from tensorflow.keras.optimizers import RMSprop, Adam, Adagrad\n",
        "from keras.regularizers import l2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from scipy.stats import shapiro\n",
        "from time import time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giM0KfkZux_n",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Model creation\n",
        "#from tensorflow.keras.models import Sequential\n",
        "#from tensorflow.keras.layers import LSTM, Dense, Bidirectional, Flatten, Conv1D, Dropout\n",
        "#from tensorflow.keras.optimizers import RMSprop, Adam, Adagrad\n",
        "#from keras.regularizers import l2\n",
        "#import tensorflow as tf\n",
        "#from time import time\n",
        "#from tensorflow.keras.utils import plot_model\n",
        "\n",
        "\n",
        "# Loss: the RMSE (root mean squared error) will be used for training because it allows to have errors in the same units as each variable.\n",
        "def root_mean_squared_error(y_true, y_pred):\n",
        "    rmse = tf.math.sqrt(tf.math.reduce_mean(tf.square(y_pred-y_true)))\n",
        "    return rmse\n",
        "\n",
        "font_size_ticks = 20\n",
        "mae = MeanAbsoluteError()\n",
        "mape = MeanAbsolutePercentageError()\n",
        "# range number of runs usually 10\n",
        "for jp in range(1):\n",
        "\n",
        "    index = 0\n",
        "\n",
        "    print('RUN N°', jp)\n",
        "\n",
        "    for (x_tr_s, y_tr_s), (x_vl_s, y_vl_s), (x_ts_s, y_ts_s) in zip(tr_s_ds.values(), vl_s_ds.values(), ts_s_ds.values()):\n",
        "\n",
        "      print('--' * 20)\n",
        "      print(f'Iteration {index}: {x_tr_s.shape}, {x_ts_s.shape}')\n",
        "      print('--' * 20)\n",
        "\n",
        "      scaler = scalers[index]\n",
        "      tr_cut_date, vl_cut_date, ts_cut_date = cut_dates[index]\n",
        "\n",
        "      tr, vl, ts = datasets[index]\n",
        "      x_ts, y_true = ts_datasets[index]\n",
        "\n",
        "      for i, col in enumerate(df.columns):\n",
        "\n",
        "        start = time()\n",
        "\n",
        "        print(f'Training BI-GRU... for {col}')\n",
        "\n",
        "        model = Sequential([\n",
        "            Bidirectional(GRU(128, input_shape=(x_tr_ds.shape[1], 1), return_sequences = True)),\n",
        "            Bidirectional(GRU(64, return_sequences = False)),\n",
        "            Dense(1, activation='linear')\n",
        "        ])\n",
        "\n",
        "\n",
        "        model.compile(optimizer = RMSprop(5e-5), loss = root_mean_squared_error, metrics = [MeanAbsoluteError(), MeanAbsolutePercentageError()])\n",
        "\n",
        "        # Training (approximately 1 min using GPU)\n",
        "\n",
        "        EPOCHS = 200\n",
        "        BATCH_SIZE = 256\n",
        "\n",
        "        story = model.fit(\n",
        "            x = np.expand_dims(x_tr_s[:,:,i], axis=-1),\n",
        "            y = np.expand_dims(y_tr_s[:,:,i], axis=-1),\n",
        "            batch_size = BATCH_SIZE,\n",
        "            epochs = EPOCHS,\n",
        "            validation_data = (np.expand_dims(x_vl_s[:,:,i], axis=-1), np.expand_dims(y_vl_s[:,:,i], axis=-1)),\n",
        "            verbose=2,\n",
        "            callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)],\n",
        "        )\n",
        "\n",
        "        finish = time() - start\n",
        "\n",
        "        plt.figure(figsize=(10, 7))\n",
        "        plt.subplot(1, 1, 1)\n",
        "        plt.plot(story.history['loss'], label='RMSE train', linewidth=line_width)\n",
        "        plt.plot(story.history['val_loss'],label='RMSE val', linewidth=line_width)\n",
        "        plt.title(f'Iteration {index} - {col}', fontsize=28)\n",
        "        plt.xlabel('Epoch', fontsize=20)\n",
        "        plt.ylabel('RMSE', fontsize=20)\n",
        "        plt.xticks(fontsize=font_size_ticks)\n",
        "        plt.yticks(fontsize=font_size_ticks)\n",
        "        plt.tight_layout()\n",
        "        #plt.legend(fontsize=20);\n",
        "\n",
        "        plt.grid(False)\n",
        "        plt.savefig(f'{data_path}/BiGru/metrics_result/iteration_{index}_{col.replace(\"/\", \"_\")}_rmse.pdf')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        plt.figure(figsize=(10, 7))\n",
        "        plt.subplot(1, 1, 1)\n",
        "        plt.plot(story.history['mean_absolute_error'], label='MAE train', linewidth=line_width)\n",
        "        plt.plot(story.history['val_mean_absolute_error'],label='MAE val', linewidth=line_width)\n",
        "        plt.title(f'Iteration {index} - {col}',  fontsize=28)\n",
        "        plt.xlabel('Epoch', fontsize=20)\n",
        "        plt.ylabel('MAE', fontsize=20)\n",
        "        plt.xticks(fontsize=font_size_ticks)\n",
        "        plt.yticks(fontsize=font_size_ticks)\n",
        "        plt.tight_layout()\n",
        "        #plt.legend(fontsize=22);\n",
        "\n",
        "        plt.grid(False)\n",
        "        plt.savefig(f'{data_path}/BiGru/metrics_result/Iteration_{index}_{col.replace(\"/\", \"_\")}_mae.pdf')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        plt.figure(figsize=(10, 7))\n",
        "        plt.subplot(1, 1, 1)\n",
        "        plt.plot(story.history['mean_absolute_percentage_error'], label='MAPE train', linewidth=line_width)\n",
        "        plt.plot(story.history['val_mean_absolute_percentage_error'],label='MAPE val', linewidth=line_width)\n",
        "        plt.title(f'Iteration {index} - {col}', fontsize=28)\n",
        "        plt.xlabel('Epoch', fontsize=20)\n",
        "        plt.ylabel('MAPE', fontsize=20)\n",
        "        plt.xticks(fontsize=font_size_ticks)\n",
        "        plt.yticks(fontsize=font_size_ticks)\n",
        "        plt.tight_layout()\n",
        "        #plt.legend(fontsize=22);\n",
        "\n",
        "        plt.grid(False)\n",
        "        plt.savefig(f'{data_path}/BiGru/metrics_result/iteration_{index}_{col.replace(\"/\", \"_\")}_mape.pdf')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        story = None\n",
        "\n",
        "        y_ts_predict = scaler[i].inverse_transform(model.predict(x_ts_s[:,:,i], verbose=0))\n",
        "\n",
        "        ts_predict = pd.DataFrame(y_ts_predict[:, 0], index=ts[:-1].index)\n",
        "\n",
        "        mae = MeanAbsoluteError()\n",
        "        mape = MeanAbsolutePercentageError()\n",
        "\n",
        "        mae.update_state(y_true[:, :, i], y_ts_predict)\n",
        "        mape.update_state(y_true[:, :, i], y_ts_predict)\n",
        "\n",
        "        i2, b2 = y_ts_predict.shape\n",
        "\n",
        "    # print(f'VARIABLE ==> [{col}]\\n')\n",
        "    # print(f'RMSE -> {round(root_mean_squared_error(y_true[:, :, i], y_ts_predict).numpy(), 3)} \\n')\n",
        "    # print(f'MAE ->  {round(mae.result().numpy(), 3)} \\n')\n",
        "    # print(f'MAPE -> { round(mape.result().numpy(), 3)} \\n')\n",
        "    # print(f'SW statistic: -> {shapiro(abs(y_ts_predict.reshape(i2, b2))).statistic} \\n')\n",
        "    # print(f'TRAIN TIME -> {round(finish, 3)} \\n')\n",
        "\n",
        "\n",
        "        with open(f'{data_path}/BiGru/metrics_result/{jp}_bi-gru_Iteration_{index}_results_ts.txt', 'a') as file:\n",
        "          file.write(f'VARIABLE ; [{col}]\\n')\n",
        "          file.write(f'RMSE ; {round(root_mean_squared_error(y_true[:, :, i], y_ts_predict).numpy(), 3)} \\n')\n",
        "          file.write(f'MAE ;  {round(mae.result().numpy(), 3)} \\n')\n",
        "          file.write(f'MAPE ; { round(mape.result().numpy(), 3)} \\n')\n",
        "\n",
        "          #result = (abs(y_ts_predict.reshape(i2, b2)))\n",
        "          #file.write(f'SW statistic: -> {result.statistic}, p-value: {result.pvalue} \\n')\n",
        "\n",
        "          file.write(f'SW statistic: ; {shapiro(abs(y_ts_predict.reshape(i2, b2))).statistic} \\n')\n",
        "          file.write(f'TRAIN TIME ; {round(finish, 3)} \\n')\n",
        "\n",
        "\n",
        "        plt.figure(figsize=(10, 7))\n",
        "        plt.subplot(1, 1, 1)\n",
        "        plt.plot(ts.index, ts[[col]].values,  label='Real', linewidth=line_width)\n",
        "        plt.plot(ts_predict.index, ts_predict.values, label='Predicted BiGRU', linewidth=line_width)\n",
        "        plt.title(f'Iteration {index} - {col}', fontdict={'fontsize': 28})\n",
        "        plt.xlabel('Time', fontsize=15)\n",
        "        plt.ylabel('Value', fontsize=20)\n",
        "\n",
        "        plt.xticks(fontsize=font_size_ticks)\n",
        "        plt.yticks(fontsize=font_size_ticks)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        plt.grid(False)\n",
        "        #plt.legend(fontsize=22)\n",
        "        plt.savefig(f'{data_path}/BiGru/test_result/iteration_{index}_{col.replace(\"/\", \"_\")}_test_result.pdf')\n",
        "        plt.show()\n",
        "\n",
        "        fig, axe = plt.subplots(figsize=(10, 7))\n",
        "        box = axe.boxplot(ts_predict.values.shape - ts[[col]].values[:-1], notch=True, vert=True, autorange=True, sym=\"\")\n",
        "        for box in box['boxes']:\n",
        "          box.set_color('b')\n",
        "          axe.set_title(f'Iteration {index} - {col}', fontsize=23)\n",
        "          axe.tick_params(labelsize=23)\n",
        "          plt.tight_layout()\n",
        "          plt.savefig(f'{data_path}/BiGru/test_result/iteration-{index}_{cut_col_name(col)}_test_result_box_plot.pdf')\n",
        "        # break\n",
        "      # break\n",
        "      index += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKqQS1fl6Mhg"
      },
      "source": [
        "# 3. LSTM Model Building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yd5Tngac6Mhg"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8EV28Iwdt9Xc"
      },
      "outputs": [],
      "source": [
        "# Loss: the RMSE (root mean squared error) will be used for training because it allows to have errors in the same units as each variable.\n",
        "def root_mean_squared_error(y_true, y_pred):\n",
        "    rmse = tf.math.sqrt(tf.math.reduce_mean(tf.square(y_pred-y_true)))\n",
        "    return rmse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIJwx7316Mhj"
      },
      "outputs": [],
      "source": [
        "dict_variables_results = {}\n",
        "\n",
        "for i, col in enumerate(df.columns):\n",
        "  dict_variables_results[col] = {\n",
        "      \"rmse\": 0,\n",
        "      \"mae\": 0,\n",
        "      \"mape\": 0,\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpynICJvX_B5",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Model creation\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Bidirectional, Flatten, Conv1D, Dropout\n",
        "from tensorflow.keras.optimizers import RMSprop, Adam, Adagrad\n",
        "from keras.regularizers import l2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from scipy.stats import shapiro\n",
        "from time import time\n",
        "\n",
        "# Loss: the RMSE (root mean squared error) will be used for training because it allows to have errors in the same units as each variable.\n",
        "def root_mean_squared_error(y_true, y_pred):\n",
        "    rmse = tf.math.sqrt(tf.math.reduce_mean(tf.square(y_pred-y_true)))\n",
        "    return rmse\n",
        "\n",
        "# range number of runs usually 10\n",
        "for jp in range(1):\n",
        "\n",
        "  index = 0\n",
        "\n",
        "  print('RUN N°', jp)\n",
        "\n",
        "  for (x_tr_s, y_tr_s), (x_vl_s, y_vl_s), (x_ts_s, y_ts_s) in zip(tr_s_ds.values(), vl_s_ds.values(), ts_s_ds.values()):\n",
        "\n",
        "    print('--' * 20)\n",
        "    print(f'Iteration {index}: {x_tr_s.shape}, {x_ts_s.shape}')\n",
        "    print('--' * 20)\n",
        "\n",
        "    scaler = scalers[index]\n",
        "    tr_cut_date, vl_cut_date, ts_cut_date = cut_dates[index]\n",
        "\n",
        "    tr, vl, ts = datasets[index]\n",
        "    x_ts, y_true = ts_datasets[index]\n",
        "\n",
        "    for i, col in enumerate(df.columns):\n",
        "\n",
        "      start = time()\n",
        "\n",
        "      print(f'Training LSTM... for {col}')\n",
        "\n",
        "      model = Sequential([\n",
        "          LSTM(128, input_shape=(x_tr_ds.shape[1], 1), return_sequences = True,),\n",
        "          Dropout(0.1),\n",
        "          LSTM(128, return_sequences = True),\n",
        "          LSTM(64, return_sequences = False),\n",
        "          Dense(16, activation='relu'),\n",
        "          Dense(1, activation='linear')]\n",
        "      )\n",
        "\n",
        "\n",
        "      model.compile(optimizer = RMSprop(5e-5), loss = root_mean_squared_error, metrics = [MeanAbsoluteError(), MeanAbsolutePercentageError()])\n",
        "\n",
        "      # Training (approximately 1 min using GPU)\n",
        "\n",
        "      EPOCHS = 250\n",
        "      BATCH_SIZE = 256\n",
        "\n",
        "      story = model.fit(\n",
        "          x = np.expand_dims(x_tr_s[:,:,i], axis=-1),\n",
        "          y = np.expand_dims(y_tr_s[:,:,i], axis=-1),\n",
        "          batch_size = BATCH_SIZE,\n",
        "          epochs = EPOCHS,\n",
        "          validation_data = (np.expand_dims(x_vl_s[:,:,i], axis=-1), np.expand_dims(y_vl_s[:,:,i], axis=-1)),\n",
        "          verbose=2,\n",
        "          callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)],\n",
        "      )\n",
        "\n",
        "      finish = time() - start\n",
        "\n",
        "      plt.figure(figsize=(10, 7))\n",
        "      plt.subplot(1, 1, 1)\n",
        "      plt.plot(story.history['loss'], label='RMSE train', linewidth=line_width)\n",
        "      plt.plot(story.history['val_loss'],label='RMSE val', linewidth=line_width)\n",
        "      plt.title(f'Iteration {index} - {col}', fontsize=28)\n",
        "      plt.xlabel('Epoch', fontsize=20)\n",
        "      plt.ylabel('RMSE', fontsize=20)\n",
        "      plt.xticks(fontsize=font_size_ticks)\n",
        "      plt.yticks(fontsize=font_size_ticks)\n",
        "      plt.tight_layout()\n",
        "      #plt.legend(fontsize=22);\n",
        "\n",
        "      plt.grid(False)\n",
        "      plt.savefig(f'{data_path}/LSTM/metrics_result/iteration_{index}_{col.replace(\"/\", \"_\")}_rmse.pdf')\n",
        "      plt.tight_layout()\n",
        "      plt.show()\n",
        "\n",
        "      plt.figure(figsize=(10, 7))\n",
        "      plt.subplot(1, 1, 1)\n",
        "      plt.plot(story.history['mean_absolute_error'], label='MAE train', linewidth=line_width)\n",
        "      plt.plot(story.history['val_mean_absolute_error'],label='MAE val', linewidth=line_width)\n",
        "      plt.title(f'Iteration {index} - {col}',  fontsize=28)\n",
        "      plt.xlabel('Epoch', fontsize=20)\n",
        "      plt.ylabel('MAE', fontsize=20)\n",
        "      plt.xticks(fontsize=font_size_ticks)\n",
        "      plt.yticks(fontsize=font_size_ticks)\n",
        "      plt.tight_layout()\n",
        "      #plt.legend(fontsize=22);\n",
        "\n",
        "      plt.grid(False)\n",
        "      plt.savefig(f'{data_path}/LSTM/metrics_result/iteration_{index}_{col.replace(\"/\", \"_\")}_mae.pdf')\n",
        "      plt.tight_layout()\n",
        "      plt.show()\n",
        "\n",
        "      plt.figure(figsize=(10, 7))\n",
        "      plt.subplot(1, 1, 1)\n",
        "      plt.plot(story.history['mean_absolute_percentage_error'], label='MAPE train', linewidth=line_width)\n",
        "      plt.plot(story.history['val_mean_absolute_percentage_error'],label='MAPE val', linewidth=line_width)\n",
        "      plt.title(f'Iteration {index} - {col}', fontsize=28)\n",
        "      plt.xlabel('Epoch', fontsize=20)\n",
        "      plt.ylabel('MAPE', fontsize=20)\n",
        "      plt.xticks(fontsize=font_size_ticks)\n",
        "      plt.yticks(fontsize=font_size_ticks)\n",
        "      plt.tight_layout()\n",
        "      #plt.legend(fontsize=22);\n",
        "\n",
        "      plt.grid(False)\n",
        "      plt.savefig(f'{data_path}/LSTM/metrics_result/iteration_{index}_{col.replace(\"/\", \"_\")}_mape.pdf')\n",
        "      plt.tight_layout()\n",
        "      plt.show()\n",
        "\n",
        "      story = None\n",
        "\n",
        "      y_ts_predict = scaler[i].inverse_transform(model.predict(x_ts_s[:,:,i], verbose=0))\n",
        "\n",
        "      ts_predict = pd.DataFrame(y_ts_predict[:, 0], index=ts[:-1].index)\n",
        "\n",
        "      mae = MeanAbsoluteError()\n",
        "      mape = MeanAbsolutePercentageError()\n",
        "\n",
        "      mae.update_state(y_true[:, :, i], ts_predict.values)\n",
        "      mape.update_state(y_true[:, :, i], ts_predict.values)\n",
        "\n",
        "      i2, b2 = y_ts_predict.shape\n",
        "\n",
        "      # with open(f'./lstm-fold_{index}_results_ts.txt', 'a') as file:\n",
        "      with open(f'{data_path}/LSTM/metrics_result/{jp}_Iteration_{index}_results_ts.txt', 'a') as file:\n",
        "        file.write(f'VARIABLE ==> [{col}]\\n')\n",
        "        file.write(f'RMSE -> {round(root_mean_squared_error(y_true[:, :, i], ts_predict.values).numpy(), 3)} \\n')\n",
        "        file.write(f'MAE ->  {round(mae.result().numpy(), 3)} \\n')\n",
        "        file.write(f'MAPE -> { round(mape.result().numpy(), 3)} \\n')\n",
        "        file.write(f'SW statistic: -> {shapiro(abs(y_ts_predict.reshape(i2, b2))).statistic} \\n')\n",
        "        file.write(f'TRAIN TIME -> {round(finish, 3)} \\n')\n",
        "\n",
        "      plt.figure(figsize=(10, 7))\n",
        "      plt.subplot(1, 1, 1)\n",
        "      plt.plot(ts.index, ts[[col]].values, label='Real', linewidth=line_width)\n",
        "      plt.plot(ts_predict.index, ts_predict.values, label='Predicted LSTM', linewidth=line_width)\n",
        "      plt.title(f'Iteration {index} - {col}', fontdict={'fontsize': 28})\n",
        "      plt.xlabel('Time', fontsize=20)\n",
        "      plt.ylabel('Value', fontsize=20)\n",
        "\n",
        "      plt.xticks(fontsize=font_size_ticks)\n",
        "      plt.yticks(fontsize=font_size_ticks)\n",
        "      plt.grid(False)\n",
        "      #plt.legend(fontsize=22)\n",
        "      plt.savefig(f'{data_path}/LSTM/test_result/iteration_{index}_{col.replace(\"/\", \"_\")}_test_result.pdf')\n",
        "      plt.show()\n",
        "\n",
        "      fig, axe = plt.subplots(figsize=(10, 7))\n",
        "      box = axe.boxplot(ts_predict.values - ts[[col]][:-1].values, notch=True, vert=True, autorange=True, sym=\"\")\n",
        "      for box in box['boxes']:\n",
        "        box.set_color('b')\n",
        "        axe.set_title(f'Iteration {index} - {col}', fontsize=23)\n",
        "        axe.tick_params(labelsize=23)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{data_path}/LSTM/test_result/iteration-{index}_{cut_col_name(col)}_test_result_box_plot.pdf')\n",
        "      # break\n",
        "\n",
        "    index += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XhZLMMMTirJ"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4z1z1NrY_IYl"
      },
      "source": [
        "# 3. ARIMA Model Building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nk2enkq9DSpn"
      },
      "outputs": [],
      "source": [
        "!pip install pmdarima"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_KVUj9mcug5"
      },
      "outputs": [],
      "source": [
        "def metrics_values(y_true, y_pred):\n",
        "\n",
        "  for index, column in enumerate(tr.columns):\n",
        "    mae = MeanAbsoluteError()\n",
        "    mape = MeanAbsolutePercentageError()\n",
        "    mae.update_state(y_true[:, :, index], y_pred[:, :, index])\n",
        "    mape.update_state(y_true[:, :, index], y_pred[:, :, index])\n",
        "\n",
        "    with open('results_tr.txt', 'a') as file:\n",
        "      file.write(f'VARIABLE;[{column}]\\n')\n",
        "      file.write(f'RMSE;{round(root_mean_squared_error(y_true[:, :, index], y_pred[:, :, index]).numpy(), 3)} \\n')\n",
        "      file.write(f'MAE;{round(mae.result().numpy(), 3)} \\n')\n",
        "      file.write(f'MAPE;{ round(mape.result().numpy(), 3)} \\n')\n",
        "    # file.write(f'MASE -> { round(mase.result().numpy(), 3)} \\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVnXbF4f0V7r"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.metrics import RootMeanSquaredError, MeanAbsoluteError, MeanAbsolutePercentageError, MeanSquaredError\n",
        "from pmdarima import ARIMA, auto_arima\n",
        "from scipy.stats import shapiro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_a9tN7my1OL"
      },
      "outputs": [],
      "source": [
        "def root_mean_squared_error(y_true, y_pred):\n",
        "    rmse = tf.math.sqrt(tf.math.reduce_mean(tf.square(y_pred-y_true)))\n",
        "    return rmse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q61z-lbLkxA5"
      },
      "outputs": [],
      "source": [
        "from time import time\n",
        "\n",
        "#index = 0\n",
        "print(\"ARIMA\")\n",
        "\n",
        "#dict_variables = {}\n",
        "\n",
        "#dict_variables_results = {}\n",
        "\n",
        "#for i, col in enumerate(df.columns):\n",
        "#  dict_variables_results[col] = {\n",
        "#      \"rmse\": 0,\n",
        "#      \"mae\": 0,\n",
        "#      \"mape\": 0,\n",
        "#  }\n",
        "\n",
        "font_size_ticks = 20\n",
        "\n",
        "# range number of runs usually 10\n",
        "for jp in range(1):\n",
        "\n",
        "    index = 0\n",
        "\n",
        "    print('RUN N°', jp)\n",
        "\n",
        "    for (x_tr_s, y_tr_s), (x_vl_s, y_vl_s), (x_ts_s, y_ts_s) in zip(tr_s_ds.values(), vl_s_ds.values(), ts_s_ds.values()):\n",
        "\n",
        "        print('--' * 20)\n",
        "        print(f'Training ARIMA... for {col} {x_tr_s[:,:,i].shape} ---- {tr.index[:-1].shape}')\n",
        "        print('--' * 20)\n",
        "\n",
        "        scaler = scalers[index]\n",
        "        tr_cut_date, vl_cut_date, ts_cut_date = cut_dates[index]\n",
        "\n",
        "        tr, vl, ts = datasets[index]\n",
        "        x_ts, y_true = ts_datasets[index]\n",
        "\n",
        "        for i, col in enumerate(df.columns):\n",
        "          start = time()\n",
        "\n",
        "          print(f'Training ARIMA... for {col}')\n",
        "\n",
        "          #modelo.summary()\n",
        "\n",
        "          modelo = ARIMA(order=(1, 1, 1), seasonal_order=(1, 1, 1, 7))\n",
        "          modelo.fit(y=pd.DataFrame(x_tr_s[:,:,i], index=tr.index[:-1])) # tomo hasta la penultima fecha\n",
        "\n",
        "          #predicciones_pdmarima = modelo.predict(len(x_vl_s.shape[0]) + len(x_ts_s.shape[0]))\n",
        "\n",
        "          predicciones_pdmarima = modelo.predict(len(vl) + len(ts))\n",
        "          predicciones_pdmarima.name = 'predicciones_pdmarima'\n",
        "          pred_arima = pd.DataFrame(predicciones_pdmarima[predicciones_pdmarima.index > str(vl_cut_date)])\n",
        "\n",
        "          plt.figure(figsize=(10, 7))\n",
        "          plt.subplot(2, 1, 1)\n",
        "\n",
        "          scaled_predicted = scaler[i].inverse_transform(pred_arima.values)\n",
        "\n",
        "          finish = time() - start\n",
        "\n",
        "          # pd.DataFrame({'predict': scaler[i].inverse_transform(pred_arima.values).squeeze(), 'real': ts[[col]].values.squeeze()}, index=ts.index).to_csv('result')\n",
        "\n",
        "          plt.plot(ts.index[:-1], ts[[col]][:-1], label='Real', linewidth=line_width)\n",
        "          plt.plot(pred_arima.index, scaled_predicted, label='Estimated ARIMA - pmdarima', linewidth=line_width)\n",
        "          plt.title(f'Iteration {index} - {col}', fontdict={'fontsize': 28})\n",
        "          #plt.legend(fontsize=25)\n",
        "          plt.xlabel('Time', fontsize=30)\n",
        "          plt.ylabel('Value', fontsize=30)\n",
        "\n",
        "          plt.xticks(fontsize=font_size_ticks) # aumento de datos\n",
        "          plt.yticks(fontsize=font_size_ticks)\n",
        "\n",
        "          plt.grid(False)\n",
        "          plt.savefig(f'{data_path}/Arima/test_result/Iteration-{index}_{cut_col_name(col)}_test_result.pdf')\n",
        "          plt.tight_layout()\n",
        "          plt.show()\n",
        "\n",
        "    #      story = None\n",
        "\n",
        "          mae = MeanAbsoluteError()\n",
        "          mape = MeanAbsolutePercentageError()\n",
        "\n",
        "          mae.update_state(y_true[:, :, i], scaled_predicted)\n",
        "          mape.update_state(y_true[:, :, i], scaled_predicted)\n",
        "\n",
        "          i2, b2 = scaled_predicted.shape\n",
        "\n",
        "          with open(f'{data_path}/Arima/metrics_result/{jp}_Arima_Iteration_{index}_results_ts.txt', 'a') as file:\n",
        "          # with open(f'./AutoArima-fold_{index}_results_ts.txt', 'a') as file:\n",
        "            file.write(f'VARIABLE;[{col}]\\n')\n",
        "            file.write(f'RMSE;{round(root_mean_squared_error(y_true[:, :, i], scaled_predicted).numpy(), 3)} \\n')\n",
        "            file.write(f'MAE;{round(mae.result().numpy(), 3)} \\n')\n",
        "            file.write(f'MAPE;{ round(mape.result().numpy(), 3)} \\n')\n",
        "            file.write(f'SW statistic;{shapiro(abs(scaled_predicted.reshape(i2, b2))).statistic} \\n')\n",
        "            file.write(f'TRAIN TIME;{round(finish, 3)} \\n')\n",
        "\n",
        "          dict_variables_results[col]['rmse'] = dict_variables_results[col]['rmse'] + round(root_mean_squared_error(y_true[:, :, i], scaled_predicted).numpy(), 3)\n",
        "          dict_variables_results[col]['mae'] = dict_variables_results[col]['mae'] + round(mae.result().numpy(), 3)\n",
        "          dict_variables_results[col]['mape'] = dict_variables_results[col]['mape'] + round(mape.result().numpy(), 3)\n",
        "\n",
        "          fig, axe = plt.subplots(figsize=(10, 7))\n",
        "          box = axe.boxplot(scaled_predicted - ts[[col]][:-1], notch=True, vert=True, autorange=True, sym=\"\")\n",
        "          for box in box['boxes']:\n",
        "            box.set_color('b')\n",
        "            axe.set_title(f'Iteration {index} - {col}', fontsize=23)\n",
        "            axe.tick_params(labelsize=23)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f'{data_path}/Arima/test_result/Iteration-{index}_{cut_col_name(col)}_test_result_box_plot.pdf')\n",
        "\n",
        "\n",
        "        plt.show()\n",
        "        # break\n",
        "\n",
        "\n",
        "        for col in df.columns:\n",
        "            with open(f'{data_path}/Arima/metrics_result/{jp}_Iterations_results_summary.txt', 'a') as file:\n",
        "              file.write(f'VARIABLE; [{col}]\\n')\n",
        "              file.write(f'RMSE; {dict_variables_results[col][\"rmse\"]} \\n')\n",
        "              file.write(f'MAE; {dict_variables_results[col][\"mae\"]} \\n')\n",
        "              file.write(f'MAPE; {dict_variables_results[col][\"mape\"]} \\n')\n",
        "\n",
        "        index += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWw9sf-Qkywb"
      },
      "source": [
        "# 4. AUTO ARIMA Model Building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plUUL-2DHZ3o",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# from time import time\n",
        "\n",
        "index = 0\n",
        "\n",
        "dict_variables = {}\n",
        "\n",
        "dict_variables_results = {}\n",
        "\n",
        "#for i, col in enumerate(df.columns):\n",
        "#  dict_variables_results[col] = {\n",
        "#      \"rmse\": 0,\n",
        "#      \"mae\": 0,\n",
        "#      \"mape\": 0,\n",
        "#      \"time\": 0,\n",
        "#  }\n",
        "\n",
        "# range number of runs usually 10\n",
        "for jp in range(1):\n",
        "# for (x_tr_s, y_tr_s), (x_vl_s, y_vl_s), (x_ts_s, y_ts_s) in zip(tr_s_ds.values(), vl_s_ds.values(), ts_s_ds.values()):\n",
        "\n",
        "  for (x_tr_s, y_tr_s), (x_vl_s, y_vl_s), (x_ts_s, y_ts_s) in zip(tr_s_ds.values(), vl_s_ds.values(), ts_s_ds.values()):\n",
        "\n",
        "    print('--' * 20)\n",
        "    print(f'Training ARIMA... for {col} {x_tr_s[:,:,i].shape} ---- {tr.index[:-1].shape}')\n",
        "    print('--' * 20)\n",
        "\n",
        "    scaler = scalers[index]\n",
        "    tr_cut_date, vl_cut_date, ts_cut_date = cut_dates[index]\n",
        "\n",
        "    tr, vl, ts = datasets[index]\n",
        "    x_ts, y_true = ts_datasets[index]\n",
        "\n",
        "    for i, col in enumerate(df.columns):\n",
        "      start = time()\n",
        "\n",
        "      print(f'Training AUTOARIMA... for {col}')\n",
        "\n",
        "      modelo = auto_arima(pd.DataFrame(x_tr_s[:,:,i], index=tr.index[:-1]),\n",
        "                        m=12,               # frequency of series\n",
        "                        d=None,             # let model determine 'd'\n",
        "                        test='adf',         # use adftest to find optimal 'd'\n",
        "                        start_p=0, start_q=0, # minimum p and q\n",
        "                        max_p=12, max_q=12, # maximum p and q\n",
        "                        D=None,             # let model determine 'D'\n",
        "                        trace=True,\n",
        "                        error_action='ignore',\n",
        "                        suppress_warnings=True,\n",
        "                        stepwise=True,\n",
        "      )\n",
        "\n",
        "      modelo.summary()\n",
        "\n",
        "\n",
        "      predicciones_pdmarima = modelo.predict(len(vl) + len(ts))\n",
        "      predicciones_pdmarima.name = 'predicciones_autoarima'\n",
        "      pred_arima = pd.DataFrame(predicciones_pdmarima[predicciones_pdmarima.index > str(vl_cut_date)])\n",
        "\n",
        "      plt.figure(figsize=(18, 15))\n",
        "      plt.subplot(2, 1, 1)\n",
        "\n",
        "      scaled_predicted = scaler[i].inverse_transform(pred_arima.values)\n",
        "\n",
        "      finish = time() - start\n",
        "\n",
        "      # pd.DataFrame({'predict': scaler[i].inverse_transform(pred_arima.values).squeeze(), 'real': ts[[col]].values.squeeze()}, index=ts.index).to_csv('result')\n",
        "\n",
        "      plt.plot(ts.index[:-1], ts[[col]][:-1], label='Real', linewidth=line_width)\n",
        "      plt.plot(pred_arima.index, scaled_predicted, label='Predicted ARIMA - pmdarima', linewidth=line_width)\n",
        "      plt.title(f'Iteration {index} - {col}', fontdict={'fontsize': 28})\n",
        "      #plt.legend(fontsize=22)\n",
        "\n",
        "      plt.xticks(fontsize=20) # aumento de datos\n",
        "      plt.yticks(fontsize=20)\n",
        "\n",
        "      plt.grid(False)\n",
        "      plt.savefig(f'{data_path}/AutoArima/test_result/Iteration-{index}_{cut_col_name(col)}_test_result.pdf')\n",
        "      plt.tight_layout()\n",
        "      plt.show()\n",
        "\n",
        "      mae = MeanAbsoluteError()\n",
        "      mape = MeanAbsolutePercentageError()\n",
        "\n",
        "      mae.update_state(y_true[:, :, i], scaled_predicted)\n",
        "      mape.update_state(y_true[:, :, i], scaled_predicted)\n",
        "\n",
        "      i2, b2 = scaled_predicted.shape\n",
        "\n",
        "      with open(f'{data_path}/AutoArima/metrics_result/{jp}_Iteration_{index}_results_ts.txt', 'a') as file:\n",
        "      # with open(f'./AutoArima-fold_{index}_results_ts.txt', 'a') as file:\n",
        "        file.write(f'VARIABLE ==> [{col}]\\n')\n",
        "        file.write(f'RMSE -> {round(root_mean_squared_error(y_true[:, :, i], scaled_predicted).numpy(), 3)} \\n')\n",
        "        file.write(f'MAE ->  {round(mae.result().numpy(), 3)} \\n')\n",
        "        file.write(f'MAPE -> { round(mape.result().numpy(), 3)} \\n')\n",
        "        file.write(f'SW statistic: -> {shapiro(abs(scaled_predicted.reshape(i2, b2))).statistic} \\n')\n",
        "        file.write(f'TRAIN TIME -> {round(finish, 3)} \\n')\n",
        "\n",
        "      dict_variables_results[col]['rmse'] = dict_variables_results[col]['rmse'] + round(root_mean_squared_error(y_true[:, :, i], scaled_predicted).numpy(), 3)\n",
        "      dict_variables_results[col]['mae'] = dict_variables_results[col]['mae'] + round(mae.result().numpy(), 3)\n",
        "      dict_variables_results[col]['mape'] = dict_variables_results[col]['mape'] + round(mape.result().numpy(), 3)\n",
        "      dict_variables_results[col]['time'] = dict_variables_results[col]['time'] + round(finish, 3)\n",
        "\n",
        "      fig, axe = plt.subplots(figsize=(5, 7))\n",
        "      box = axe.boxplot(scaled_predicted - ts[[col]][:-1], notch=True, vert=True, autorange=True, sym=\"\")\n",
        "      for box in box['boxes']:\n",
        "        box.set_color('b')\n",
        "        axe.set_title(f'Iteration {index} - {col}', fontsize=23)\n",
        "        axe.tick_params(labelsize=23)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{data_path}/AutoArima/test_result/Iteration-{index}_{cut_col_name(col)}_test_result_box_plot.pdf')\n",
        "\n",
        "    index += 1\n",
        "\n",
        "    plt.show()\n",
        "    # break\n",
        "\n",
        "\n",
        "  for col in df.columns:\n",
        "    with open(f'{data_path}/AutoArima/metrics_result/{jp}_Iterations_results_summary.txt', 'a') as file:\n",
        "      file.write(f'VARIABLE , [{col}]\\n')\n",
        "      file.write(f'RMSE , {dict_variables_results[col][\"rmse\"]} \\n')\n",
        "      file.write(f'MAE , {dict_variables_results[col][\"mae\"]} \\n')\n",
        "      file.write(f'MAPE , {dict_variables_results[col][\"mape\"]} \\n')\n",
        "      file.write(f'TRAIN TIME , {dict_variables_results[col][\"time\"]} \\n')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}