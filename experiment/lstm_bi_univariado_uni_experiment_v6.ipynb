{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3195,
     "status": "ok",
     "timestamp": 1722828983339,
     "user": {
      "displayName": "Ximena Guerron",
      "userId": "14256164877379905553"
     },
     "user_tz": 300
    },
    "id": "z_36qHcKgHBl",
    "outputId": "c0ca72c0-8a7b-4533-a46b-0f0f70221ca5"
   },
   "outputs": [],
   "source": [
    "!pip install scipy\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1722828983339,
     "user": {
      "displayName": "Ximena Guerron",
      "userId": "14256164877379905553"
     },
     "user_tz": 300
    },
    "id": "BOmnJ1hjkKc4",
    "outputId": "b316c5ae-677d-4317-e453-fffbd3875bec"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1722828983340,
     "user": {
      "displayName": "Ximena Guerron",
      "userId": "14256164877379905553"
     },
     "user_tz": 300
    },
    "id": "RSOwuJ0TkLk0"
   },
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-v0_8-white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1722828983340,
     "user": {
      "displayName": "Ximena Guerron",
      "userId": "14256164877379905553"
     },
     "user_tz": 300
    },
    "id": "xvHhOk_lD8bV"
   },
   "outputs": [],
   "source": [
    "data_path = '/content/drive/MyDrive/ESTUDIO_TIME_SERIES'\n",
    "#·data_path = 'C:/Users/XXX/ESTUDIO_TIME_SERIES'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Es6L4nEckb3S"
   },
   "source": [
    "# 1. Dataset Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1722828983340,
     "user": {
      "displayName": "Ximena Guerron",
      "userId": "14256164877379905553"
     },
     "user_tz": 300
    },
    "id": "GvROsCTA8nSB"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Reshape, Flatten, GRU\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam, SGD\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "\n",
    "# df1 = pd.read_csv('cpu_percent_results_2.csv', usecols=['beginTimeSeconds', 'CPU percent'])\n",
    "# df2 = pd.read_csv('uptime_results_2.csv', usecols=['Uptime'])\n",
    "# df3 = pd.read_csv('tx_rx_results_2.csv',\n",
    "#                   usecols=['Rx packets', 'Tx packets'])\n",
    "# df4 = pd.read_csv('nb_np_results_2.csv',\n",
    "#                   usecols=['NetBytes In', 'NetBytes Out', 'NetPackets In', 'NetPackets Out'])\n",
    "# df5 = pd.read_csv('disk_memory_results_2.csv',\n",
    "#                   usecols=['Disk read/s', 'Disk write/s', 'Free Disk', 'Free Memory', 'Used Disk',\n",
    "#                            'Used Memory'], dtype='str')\n",
    "# df6 = pd.read_csv('disk_percent_results_2.csv', usecols=['Disk Used percent'])\n",
    "# df7 = pd.read_csv('memory_percent_results_2.csv', usecols=['Memory Used percent'])\n",
    "\n",
    "# df = pd.concat([df1, df2, df3, df4, df5, df6, df7], axis=1)\n",
    "# df = df2.copy()\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2598,
     "status": "ok",
     "timestamp": 1722828985930,
     "user": {
      "displayName": "Ximena Guerron",
      "userId": "14256164877379905553"
     },
     "user_tz": 300
    },
    "id": "KLX9OtOsCqq2",
    "outputId": "7e12b55e-b3b1-4752-a047-968b00878d1e"
   },
   "outputs": [],
   "source": [
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 379
    },
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1722828985930,
     "user": {
      "displayName": "Ximena Guerron",
      "userId": "14256164877379905553"
     },
     "user_tz": 300
    },
    "id": "PPqg17Gyf3sR",
    "outputId": "62022963-3c4d-49c5-bca8-095869792ea7"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv', index_col=0, parse_dates=['beginTimeSeconds'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1722828985931,
     "user": {
      "displayName": "Ximena Guerron",
      "userId": "14256164877379905553"
     },
     "user_tz": 300
    },
    "id": "B8_FfUVXrYGN",
    "outputId": "252883fd-de3c-47f9-f67d-058eae9b2dee"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 379
    },
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1722828985932,
     "user": {
      "displayName": "Ximena Guerron",
      "userId": "14256164877379905553"
     },
     "user_tz": 300
    },
    "id": "14xjAhSHn0SL",
    "outputId": "e4903021-bee7-4379-b12d-dfa1b3c24997"
   },
   "outputs": [],
   "source": [
    "new_order_columns = [\n",
    "    'Free Memory',\n",
    "    'Used Memory',\n",
    "    'Free Disk',\n",
    "    'Used Disk',\n",
    "    'Disk read/s',\n",
    "    'Disk write/s',\n",
    "    'NetBytes In',\n",
    "    'NetBytes Out',\n",
    "    'NetPackets In',\n",
    "    'NetPackets Out',\n",
    "    'Rx packets',\n",
    "    'Tx packets',\n",
    "    'CPU percent',\n",
    "    'Memory Used percent',\n",
    "    'Disk Used percent',\n",
    "    'Uptime',\n",
    "]\n",
    "df = df[new_order_columns]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1722828985933,
     "user": {
      "displayName": "Ximena Guerron",
      "userId": "14256164877379905553"
     },
     "user_tz": 300
    },
    "id": "vjoVojl-KC0x"
   },
   "outputs": [],
   "source": [
    "def cut_col_name(col_name):\n",
    "  return re.sub(\"[^a-zA-Z]\", '', col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1722828985934,
     "user": {
      "displayName": "Ximena Guerron",
      "userId": "14256164877379905553"
     },
     "user_tz": 300
    },
    "id": "lgV5krdHTUBP"
   },
   "outputs": [],
   "source": [
    "## df['beginTimeSeconds'] = pd.to_datetime(df['beginTimeSeconds'], unit='s')\n",
    "## df = df.set_index('beginTimeSeconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1722828985934,
     "user": {
      "displayName": "Ximena Guerron",
      "userId": "14256164877379905553"
     },
     "user_tz": 300
    },
    "id": "-DLgLxpHrZVu"
   },
   "outputs": [],
   "source": [
    "# cols = df.columns\n",
    "\n",
    "# N = df.shape[0]\n",
    "# plots = df[['Free Disk', 'Used Disk', 'Disk Used percent']][:N]\n",
    "# # plots.index = df['beginTimeSeconds'][0:N]\n",
    "# plots.index = df.index[:N]\n",
    "# _ = plots.plot(subplots=True, figsize=(25,25))\n",
    "# plt.legend(fontsize='xx-large')\n",
    "\n",
    "# # plt.grid(True)\n",
    "# plt.savefig('dataset.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1722828985935,
     "user": {
      "displayName": "Ximena Guerron",
      "userId": "14256164877379905553"
     },
     "user_tz": 300
    },
    "id": "bxU52hINDnxf"
   },
   "outputs": [],
   "source": [
    "fsize_labels = 23\n",
    "fsize_ticks = 25\n",
    "fsize_title = 23\n",
    "figsize = (15, 5)\n",
    "\n",
    "# for col in df.columns:\n",
    "\n",
    "#   fig, ax = plt.subplots(figsize = figsize)\n",
    "#   ax.plot(df.index, df[col].values)\n",
    "#   ax.set_title(col, fontsize=fsize_title)\n",
    "#   ax.set_xlabel('Time', fontsize=fsize_labels)\n",
    "#   ax.set_ylabel('Value', fontsize=fsize_labels)\n",
    "#   ax.tick_params(labelsize=fsize_ticks)\n",
    "#   ax.grid(False)\n",
    "\n",
    "#   plt.tight_layout()\n",
    "#   plt.savefig(f'{data_path}/DATA_COMPLETA/{cut_col_name(col)}_data.jpg')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dtiIzQmUTRHB"
   },
   "source": [
    "## 1.1. Feature management\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7_05XHDNfdKQ"
   },
   "source": [
    "It is necessary to eliminate a certain segment of the information due to its lack of temporality, since it would generate errors during training.\n",
    "The information to be predicted corresponds to data collected after July 5, 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1722828985935,
     "user": {
      "displayName": "Ximena Guerron",
      "userId": "14256164877379905553"
     },
     "user_tz": 300
    },
    "id": "cy3FP4DoCsth"
   },
   "outputs": [],
   "source": [
    "df = df[(df.index <= '2023-07-05 00:00:00')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1722828985935,
     "user": {
      "displayName": "Ximena Guerron",
      "userId": "14256164877379905553"
     },
     "user_tz": 300
    },
    "id": "pFBD45qSqV7M"
   },
   "outputs": [],
   "source": [
    "# cols = df.columns\n",
    "\n",
    "# N = df.shape[0]\n",
    "# plots = df[cols[-4:]][:N]\n",
    "# # plots.index = df['beginTimeSeconds'][0:N]\n",
    "# plots.index = df.index[0:N] # Time Variable\n",
    "# _ = plots.plot(subplots=True, figsize=(25,25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1722828985936,
     "user": {
      "displayName": "Ximena Guerron",
      "userId": "14256164877379905553"
     },
     "user_tz": 300
    },
    "id": "gCQ7bbFgIhbF"
   },
   "outputs": [],
   "source": [
    "fsize_labels = 23\n",
    "fsize_ticks = 20\n",
    "fsize_title = 23\n",
    "\n",
    "# for col in df.columns:\n",
    "\n",
    "#   fig, ax = plt.subplots(figsize = figsize)\n",
    "#   ax.plot(df.index, df[col].values)\n",
    "#   ax.set_title(col, fontsize=fsize_title)\n",
    "#   ax.set_xlabel('Time', fontsize=fsize_labels)\n",
    "#   ax.set_ylabel('Value', fontsize=fsize_labels)\n",
    "#   ax.tick_params(labelsize=fsize_ticks)\n",
    "#   ax.grid(False)\n",
    "\n",
    "#   plt.tight_layout()\n",
    "#   plt.savefig(f'{data_path}/DATA_LIMITADA/{cut_col_name(col)}_data.jpg')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMdgd4FLf9TJ"
   },
   "source": [
    "## 1.2. Descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1722828985936,
     "user": {
      "displayName": "Ximena Guerron",
      "userId": "14256164877379905553"
     },
     "user_tz": 300
    },
    "id": "FmgVZfrwcK7Q",
    "outputId": "c61a955a-1fee-49da-f84d-43688289a9fe"
   },
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "  print(f'\\nVariable [{column}]\\n', df[column].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 7557,
     "status": "ok",
     "timestamp": 1722828993464,
     "user": {
      "displayName": "Ximena Guerron",
      "userId": "14256164877379905553"
     },
     "user_tz": 300
    },
    "id": "y5WcY7-5O1Lz",
    "outputId": "9de14646-1a65-4914-e072-d9eab2d06720"
   },
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "  fig, axe = plt.subplots(figsize=(5, 7))\n",
    "  box = axe.boxplot(df[col], notch=True, vert=True, autorange=True, sym=\"\")\n",
    "  for box in box['boxes']:\n",
    "      box.set_color('b')\n",
    "      axe.set_title(col, fontsize=25)\n",
    "      axe.tick_params(labelsize=25)\n",
    "      plt.tight_layout()\n",
    "      # plt.savefig(f'{data_path}/DATA_STATISTICS/{cut_col_name(col)}_data.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNCpbNrGhHPC"
   },
   "source": [
    "## 1.3. Variability determination\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 322,
     "status": "ok",
     "timestamp": 1722828993761,
     "user": {
      "displayName": "Ximena Guerron",
      "userId": "14256164877379905553"
     },
     "user_tz": 300
    },
    "id": "kL7_CfqoVTMq",
    "outputId": "673d3da5-03a9-455d-da1c-eae0d640ed81"
   },
   "outputs": [],
   "source": [
    "print(len(df.columns))\n",
    "for column in df.columns:\n",
    "  arr = df[column].unique()\n",
    "  print(f'Column: [{column}] :: Unique data [{arr}], Total Unique Data [{len(arr)}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJ7c6mdfVCfF"
   },
   "source": [
    "### 1.3.3 Correlation matrix\n",
    "\n",
    "The correlation of variables is identified.\n",
    "A higher correlation when the values are closer to -1 and 1.\n",
    "It indicates if the variables have an impact on each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1722828993764,
     "user": {
      "displayName": "Ximena Guerron",
      "userId": "14256164877379905553"
     },
     "user_tz": 300
    },
    "id": "robgJnphdgDB",
    "outputId": "e4cdf347-36c2-49f7-ff1d-4e8cf942bf89",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "correlation_matrix = df.corr()\n",
    "\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 569
    },
    "executionInfo": {
     "elapsed": 2618,
     "status": "ok",
     "timestamp": 1722828996369,
     "user": {
      "displayName": "Ximena Guerron",
      "userId": "14256164877379905553"
     },
     "user_tz": 300
    },
    "id": "YiunafoJuHU9",
    "outputId": "c093392a-975c-4dec-accf-874aa2b331b5"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(25, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='Blues', fmt=\".2f\", linewidths=.5)\n",
    "plt.title('Correlation Matrix')\n",
    "# plt.savefig(f'/content/drive/MyDrive/ESTUDIO_TIME_SERIES/DATA_STATISTICS/cor_matrix.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vd5lyqNrlfFg"
   },
   "source": [
    "## 1.4. Data Pre-processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hliQnKgIdfkF"
   },
   "source": [
    "### 1.4.1. Partitioning in Training, Validation and Testing datasets\n",
    "\n",
    "Unlike other Deep Learning models, in the case of time series **it must be guaranteed that the partitions are generated without randomly mixing the data**.\n",
    "\n",
    "Note:\n",
    "- The training set (*train*) will be used to find the model parameters.\n",
    "- The validation set (*val*) to verify that there is no *under/over-fitting* of the model and to adjust its hyperparameters.\n",
    "- The test set (*test*) to test the best model found during training/validation.\n",
    "\n",
    "In this case we will use the same function implemented for the univariate models with the difference that instead of entering a *series* of Pandas, we will enter the complete *DataFrame*.\n",
    "\n",
    "Therefore, the function will return three *dataframes* (train, val and test):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1722828996370,
     "user": {
      "displayName": "Ximena Guerron",
      "userId": "14256164877379905553"
     },
     "user_tz": 300
    },
    "id": "nKWILYqZ-rBV"
   },
   "outputs": [],
   "source": [
    "fsize_labels = 30\n",
    "fsize_ticks = 25\n",
    "fsize_title = 30\n",
    "figsize = (15, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1722828996370,
     "user": {
      "displayName": "Ximena Guerron",
      "userId": "14256164877379905553"
     },
     "user_tz": 300
    },
    "id": "XEO-j_e5nMPa",
    "outputId": "b2930ebc-4563-4f88-ea71-be6f458ffe5c"
   },
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "# tr_ptg = 0.7\n",
    "# vl_ptg = 0.15\n",
    "# ts_ptg = 0.15\n",
    "num_folds = 4\n",
    "cut_dates = {}\n",
    "\n",
    "for i, fold in enumerate(range(0, num_folds - 1)):\n",
    "  len_data = len(df) / num_folds\n",
    "  len_train_data = int(len_data * (i + 1))\n",
    "\n",
    "  train_data = int(len_train_data * 0.8)\n",
    "  val_data = int(len_train_data * 0.2)\n",
    "\n",
    "  print('--' * 20)\n",
    "  print(f'Training - iteration {i}  has {len_train_data}')\n",
    "  tr = df[:train_data]\n",
    "  vl = df[train_data:train_data + val_data]\n",
    "  ts = df[len_train_data:len_train_data + int(len_data)]\n",
    "\n",
    "  cut_dates[i] = [tr.index[-1], vl.index[-1], ts.index[-1]]\n",
    "\n",
    "  print(f'Cut date train dataset: {tr.index[0]} - {tr.index[-1]}')\n",
    "  print(f'Cut date val dataset: {vl.index[0]} - {vl.index[-1]}')\n",
    "  print(f'Cut date test dataset: {ts.index[0]} - {ts.index[-1]}')\n",
    "  print('--' * 20)\n",
    "\n",
    "  datasets[i] = [tr, vl, ts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1722828996370,
     "user": {
      "displayName": "Ximena Guerron",
      "userId": "14256164877379905553"
     },
     "user_tz": 300
    },
    "id": "dLmL2l3426HY"
   },
   "outputs": [],
   "source": [
    "line_width = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1722828996370,
     "user": {
      "displayName": "Ximena Guerron",
      "userId": "14256164877379905553"
     },
     "user_tz": 300
    },
    "id": "TBB77LolWC0s"
   },
   "outputs": [],
   "source": [
    "n_fold = 3\n",
    "\n",
    "# for col in df.columns:\n",
    "#   for fold in range(n_fold):\n",
    "\n",
    "#     print(f'Presentando el fold {fold} para {col}')\n",
    "#     tr_ds = datasets[fold][0]\n",
    "#     val_ds = datasets[fold][1]\n",
    "#     ts_ds = datasets[fold][2]\n",
    "\n",
    "#     fig, ax = plt.subplots(figsize = figsize)\n",
    "\n",
    "#     ax.plot(tr_ds.index, tr_ds[col].values, linewidth=2)\n",
    "#     ax.plot(val_ds.index, val_ds[col].values, linewidth=2)\n",
    "#     ax.plot(ts_ds.index, ts_ds[col].values, linewidth=2)\n",
    "\n",
    "#     ax.set_title(f'Fold {fold} - {col}', fontsize=fsize_title)\n",
    "#     ax.set_xlabel('Time', fontsize=fsize_labels)\n",
    "#     ax.set_ylabel('Value', fontsize=fsize_labels)\n",
    "#     ax.tick_params(labelsize=fsize_ticks)\n",
    "#     ax.grid(False)\n",
    "#     plt.tight_layout()\n",
    "\n",
    "#     plt.savefig(f'{data_path}/SUPERVISED_DATASETS/{fold}_{cut_col_name(col)}_data.jpg')\n",
    "\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1722828996371,
     "user": {
      "displayName": "Ximena Guerron",
      "userId": "14256164877379905553"
     },
     "user_tz": 300
    },
    "id": "iSq2AIcdxQ4n"
   },
   "outputs": [],
   "source": [
    "def create_supervised_dataset(dataset, input_length, output_length):\n",
    "  X = []\n",
    "  Y = []\n",
    "  rows = dataset.shape[0]\n",
    "  for i in range(rows - input_length - output_length + 1):\n",
    "    X.append(dataset[i:i + input_length])\n",
    "    Y.append(dataset[i + input_length:i + input_length + output_length])\n",
    "  return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 416,
     "status": "ok",
     "timestamp": 1722828996766,
     "user": {
      "displayName": "Ximena Guerron",
      "userId": "14256164877379905553"
     },
     "user_tz": 300
    },
    "id": "AnHnsoUlHHF6",
    "outputId": "b71cc16e-4312-4a5a-ef5b-10857ab3d171"
   },
   "outputs": [],
   "source": [
    "tr_datasets = {}\n",
    "vl_datasets = {}\n",
    "ts_datasets = {}\n",
    "\n",
    "INPUT_LENGTH = 1\n",
    "OUTPUT_LENGTH = 1\n",
    "\n",
    "for fold, dataset in datasets.items():\n",
    "    tr, vl, ts = dataset\n",
    "    print('--' * 20)\n",
    "    print(f'Iteration {fold}: {tr.shape}, {vl.shape}, {ts.shape}')\n",
    "    print('--' * 20)\n",
    "\n",
    "    tr_datasets[fold] = create_supervised_dataset(tr.values, INPUT_LENGTH, OUTPUT_LENGTH)\n",
    "    vl_datasets[fold] = create_supervised_dataset(vl.values, INPUT_LENGTH, OUTPUT_LENGTH)\n",
    "    ts_datasets[fold] = create_supervised_dataset(ts.values, INPUT_LENGTH, OUTPUT_LENGTH)\n",
    "\n",
    "    print('Input (BATCHES x INPUT_LENGTH x FEATURES) and output sizes (BATCHES x OUTPUT_LENGTH x FEATURES)')\n",
    "    print(f'Train Set - x_tr: {tr_datasets[fold][0].shape}, y_tr: {tr_datasets[fold][1].shape}')\n",
    "    print(f'Validation Set - x_vl: {vl_datasets[fold][0].shape}, y_vl: {vl_datasets[fold][1].shape}')\n",
    "    print(f'Test Set - x_ts: {ts_datasets[fold][0].shape}, y_ts: {ts_datasets[fold][1].shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1722828996766,
     "user": {
      "displayName": "Ximena Guerron",
      "userId": "14256164877379905553"
     },
     "user_tz": 300
    },
    "id": "13SdoFggi_7Z"
   },
   "outputs": [],
   "source": [
    "def escalar_dataset(data_input):\n",
    "    NFEATS = data_input['x_tr'].shape[2]\n",
    "\n",
    "    # Generate list with \"scalers\" (1 for each input covariate)\n",
    "    scalers = [MinMaxScaler(feature_range=(-1,1)) for i in range(NFEATS)]\n",
    "    print(len(scalers), NFEATS)\n",
    "\n",
    "    # Arreglos que contendrán los datasets escalados\n",
    "    x_tr_s = np.zeros(data_input['x_tr'].shape)\n",
    "    x_vl_s = np.zeros(data_input['x_vl'].shape)\n",
    "    x_ts_s = np.zeros(data_input['x_ts'].shape)\n",
    "    y_tr_s = np.zeros(data_input['y_tr'].shape)\n",
    "    y_vl_s = np.zeros(data_input['y_vl'].shape)\n",
    "    y_ts_s = np.zeros(data_input['y_ts'].shape)\n",
    "\n",
    "    x_tr_ds, y_tr_ds = data_input['x_tr'], data_input['y_tr']\n",
    "    x_val_ds, y_val_ds = data_input['x_vl'], data_input['y_vl']\n",
    "    x_ts_ds, y_ts_ds = data_input['x_ts'], data_input['y_ts']\n",
    "\n",
    "    # Scaling: the min/max of the training set will be used to scale all the datasets.\n",
    "\n",
    "    # Scaling Xs\n",
    "    for i in range(NFEATS):\n",
    "        x_tr_s[:,:,i] = scalers[i].fit_transform(x_tr_ds[:,:,i])\n",
    "        x_ts_s[:,:,i] = scalers[i].transform(x_ts_ds[:,:,i])\n",
    "        x_vl_s[:,:,i] = scalers[i].transform(x_val_ds[:,:,i])\n",
    "\n",
    "        y_tr_s[:,:,i] = scalers[i].transform(y_tr_ds[:,:,i])\n",
    "        y_vl_s[:,:,i] = scalers[i].transform(y_val_ds[:,:,i])\n",
    "        y_ts_s[:,:,i] = scalers[i].transform(y_ts_ds[:,:,i])\n",
    "\n",
    "\n",
    "    # Create output dictionary\n",
    "    data_scaled = {\n",
    "        'x_tr_s': x_tr_s, 'y_tr_s': y_tr_s,\n",
    "        'x_vl_s': x_vl_s, 'y_vl_s': y_vl_s,\n",
    "        'x_ts_s': x_ts_s, 'y_ts_s': y_ts_s,\n",
    "    }\n",
    "\n",
    "    return data_scaled, scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1722828996766,
     "user": {
      "displayName": "Ximena Guerron",
      "userId": "14256164877379905553"
     },
     "user_tz": 300
    },
    "id": "9G-T7PdsjAlR",
    "outputId": "d73627c7-25e4-4cd5-a9a0-09731c04df27"
   },
   "outputs": [],
   "source": [
    "tr_s_ds, vl_s_ds, ts_s_ds = {}, {}, {}\n",
    "scalers = {}\n",
    "\n",
    "index = 0\n",
    "\n",
    "for (x_tr_ds, y_tr_ds), (x_val_ds, y_val_ds), (x_ts_ds, y_ts_ds) in zip(tr_datasets.values(), vl_datasets.values(), ts_datasets.values()):\n",
    "\n",
    "    print('--' * 20)\n",
    "    print(f'Iteration {index}: {x_tr_ds.shape}, {x_val_ds.shape}, {x_ts_ds.shape}')\n",
    "    print('--' * 20)\n",
    "\n",
    "    data_in = {\n",
    "        'x_tr': x_tr_ds, 'y_tr': y_tr_ds,\n",
    "        'x_vl': x_val_ds, 'y_vl': y_val_ds,\n",
    "        'x_ts': x_ts_ds, 'y_ts': y_ts_ds,\n",
    "    }\n",
    "\n",
    "    # Y scalar (specifying the column with the variable to predict)\n",
    "    data_s, scalers_result = escalar_dataset(data_in,)\n",
    "\n",
    "    tr_s_ds[index] = data_s['x_tr_s'], data_s['y_tr_s']\n",
    "    vl_s_ds[index] = data_s['x_vl_s'], data_s['y_vl_s']\n",
    "    ts_s_ds[index] = data_s['x_ts_s'], data_s['y_ts_s']\n",
    "    scalers[index] = scalers_result\n",
    "\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1722828996767,
     "user": {
      "displayName": "Ximena Guerron",
      "userId": "14256164877379905553"
     },
     "user_tz": 300
    },
    "id": "_fsR6fsbldup"
   },
   "outputs": [],
   "source": [
    "# And let's generate a violin plot to see the distribution of values in each covariate (input) and in the variable to predict (output).\n",
    "# index = 0\n",
    "# for (x_tr_s, y_tr_s), (x_vl_s, y_vl_s), (x_ts_s, y_ts_s) in zip(tr_s_ds.values(), vl_s_ds.values(), ts_s_ds.values()):\n",
    "#   print('--' * 20)\n",
    "#   print(f'Fold {index}: {x_tr_s.shape}, {x_vl_s.shape}, {x_ts_s.shape}')\n",
    "#   print('--' * 20)\n",
    "#   index += 1\n",
    "#   for i, col in enumerate(df.columns):\n",
    "#     plt.figure(figsize=(10, 8))\n",
    "\n",
    "#     plt.subplot(1, 1, 1)\n",
    "#     plt.violinplot(dataset=x_tr_s[:,:,i].flatten(), positions=[0])\n",
    "#     plt.violinplot(dataset=x_vl_s[:,:,i].flatten(), positions=[0])\n",
    "#     plt.violinplot(dataset=x_ts_s[:,:,i].flatten(), positions=[0])\n",
    "#     plt.title(f'Fold {index} - {col}', fontdict={'fontsize': 20})\n",
    "\n",
    "#     plt.xticks(fontsize=20)\n",
    "#     plt.yticks(fontsize=20)\n",
    "\n",
    "#     plt.grid(False)\n",
    "#     plt.savefig(f'{data_path}/DATA_STATISTICS/fold-{index}_{cut_col_name(col)}_violin.jpg')\n",
    "#     # plt.savefig(f'{col.replace(\"/\", \"_\")}_frecuencia.png')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYmEzZiI_Kqc"
   },
   "source": [
    "# 2. BI-GRU Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1722828996767,
     "user": {
      "displayName": "Ximena Guerron",
      "userId": "14256164877379905553"
     },
     "user_tz": 300
    },
    "id": "54w3pz9AYafx"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import RootMeanSquaredError, MeanAbsoluteError, MeanAbsolutePercentageError, MeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gpus = tf.config.list_physical_devices('GPU')\n",
    "#if gpus:\n",
    "#  try:\n",
    "#    # Currently, memory growth needs to be the same across GPUs\n",
    "#    for gpu in gpus:\n",
    "#      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "#    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "#    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "#  except RuntimeError as e:\n",
    "#    # Memory growth must be set before GPUs have been initialized\n",
    "#    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1722828996767,
     "user": {
      "displayName": "Ximena Guerron",
      "userId": "14256164877379905553"
     },
     "user_tz": 300
    },
    "id": "YmK8SOgL9340"
   },
   "outputs": [],
   "source": [
    "dict_variables_results = {}\n",
    "\n",
    "for i, col in enumerate(df.columns):\n",
    "  dict_variables_results[col] = {\n",
    "      \"rmse\": 0,\n",
    "      \"mae\": 0,\n",
    "      \"mape\": 0,\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1722828996768,
     "user": {
      "displayName": "Ximena Guerron",
      "userId": "14256164877379905553"
     },
     "user_tz": 300
    },
    "id": "-YHawTqM8rGM"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Bidirectional, Flatten, Conv1D, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam, Adagrad\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from scipy.stats import shapiro\n",
    "from time import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "giM0KfkZux_n",
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model creation\n",
    "\n",
    "\n",
    "# Loss: the RMSE (root mean squared error) will be used for training because it allows to have errors in the same units as each variable.\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    rmse = tf.math.sqrt(tf.math.reduce_mean(tf.square(y_pred-y_true)))\n",
    "    return rmse\n",
    "\n",
    "font_size_ticks = 20\n",
    "mae = MeanAbsoluteError()\n",
    "mape = MeanAbsolutePercentageError()\n",
    "\n",
    "for jp in range(10):\n",
    "    \n",
    "    index = 0\n",
    "    \n",
    "    print('RUN N°', jp)\n",
    "\n",
    "    for (x_tr_s, y_tr_s), (x_vl_s, y_vl_s), (x_ts_s, y_ts_s) in zip(tr_s_ds.values(), vl_s_ds.values(), ts_s_ds.values()):\n",
    "\n",
    "      print('--' * 20)\n",
    "      print(f'Iteration {index}: {x_tr_s.shape}, {x_ts_s.shape}')\n",
    "      print('--' * 20)\n",
    "    \n",
    "      scaler = scalers[index]\n",
    "      tr_cut_date, vl_cut_date, ts_cut_date = cut_dates[index]\n",
    "    \n",
    "      tr, vl, ts = datasets[index]\n",
    "      x_ts, y_true = ts_datasets[index]\n",
    "    \n",
    "      for i, col in enumerate(df.columns):\n",
    "    \n",
    "        start = time()\n",
    "    \n",
    "        print(f'Training BI-GRU... for {col}')\n",
    "    \n",
    "        model = Sequential([\n",
    "            Bidirectional(GRU(128, input_shape=(x_tr_ds.shape[1], 1), return_sequences = True)),\n",
    "            Bidirectional(GRU(64, return_sequences = False)),\n",
    "            Dense(1, activation='linear')\n",
    "        ])\n",
    "    \n",
    "    \n",
    "        model.compile(optimizer = RMSprop(5e-5), loss = root_mean_squared_error, metrics = [MeanAbsoluteError(), MeanAbsolutePercentageError()])\n",
    "    \n",
    "        # Training (approximately 1 min using GPU)\n",
    "    \n",
    "        EPOCHS = 250\n",
    "        BATCH_SIZE = 256\n",
    "\n",
    "        story = model.fit(\n",
    "            x = np.expand_dims(x_tr_s[:,:,i], axis=-1),\n",
    "            y = np.expand_dims(y_tr_s[:,:,i], axis=-1),\n",
    "            batch_size = BATCH_SIZE,\n",
    "            epochs = EPOCHS,\n",
    "            validation_data = (np.expand_dims(x_vl_s[:,:,i], axis=-1), np.expand_dims(y_vl_s[:,:,i], axis=-1)),\n",
    "            verbose=2,\n",
    "            callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)],\n",
    "        )\n",
    "    \n",
    "        finish = time() - start\n",
    "\n",
    "        plt.figure(figsize=(15, 7))\n",
    "        plt.subplot(1, 1, 1)\n",
    "        plt.plot(story.history['loss'], label='RMSE train', linewidth=line_width)\n",
    "        plt.plot(story.history['val_loss'],label='RMSE test', linewidth=line_width)\n",
    "        #plt.title(f'Iteration {index} - {col}', fontsize=28)\n",
    "        plt.xlabel('Epoch', fontsize=fsize_labels)\n",
    "        plt.ylabel('RMSE', fontsize=fsize_labels)\n",
    "        plt.xticks(fontsize=font_size_ticks)\n",
    "        plt.yticks(fontsize=font_size_ticks)\n",
    "        plt.tight_layout()\n",
    "        plt.legend(fontsize=25);\n",
    "    \n",
    "        plt.grid(False)\n",
    "        plt.savefig(f'{data_path}/BiGru/metrics_result/iteration_{index}_{col.replace(\"/\", \"_\")}_rmse.pdf')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "        plt.figure(figsize=(15, 7))\n",
    "        plt.subplot(1, 1, 1)\n",
    "        plt.plot(story.history['mean_absolute_error'], label='MAE train', linewidth=line_width)\n",
    "        plt.plot(story.history['val_mean_absolute_error'],label='MAE test', linewidth=line_width)\n",
    "        #plt.title(f'Iteration {index} - {col}',  fontsize=28)\n",
    "        plt.xlabel('Epoch', fontsize=fsize_labels)\n",
    "        plt.ylabel('MAE', fontsize=fsize_labels)\n",
    "        plt.xticks(fontsize=font_size_ticks)\n",
    "        plt.yticks(fontsize=font_size_ticks)\n",
    "        plt.tight_layout()\n",
    "        plt.legend(fontsize=25);\n",
    "    \n",
    "        plt.grid(False)\n",
    "        plt.savefig(f'{data_path}/BiGru/metrics_result/Iteration_{index}_{col.replace(\"/\", \"_\")}_mae.pdf')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(15, 7))\n",
    "        plt.subplot(1, 1, 1)\n",
    "        plt.plot(story.history['mean_absolute_percentage_error'], label='MAPE train', linewidth=line_width)\n",
    "        plt.plot(story.history['val_mean_absolute_percentage_error'],label='MAPE test', linewidth=line_width)\n",
    "        #plt.title(f'Iteration {index} - {col}', fontsize=28)\n",
    "        plt.xlabel('Epoch', fontsize=fsize_labels)\n",
    "        plt.ylabel('MAPE', fontsize=fsize_labels)\n",
    "        plt.xticks(fontsize=font_size_ticks)\n",
    "        plt.yticks(fontsize=font_size_ticks)\n",
    "        plt.tight_layout()\n",
    "        plt.legend(fontsize=25);\n",
    "\n",
    "        plt.grid(False)\n",
    "        plt.savefig(f'{data_path}/BiGru/metrics_result/iteration_{index}_{col.replace(\"/\", \"_\")}_mape.pdf')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "        story = None\n",
    "\n",
    "        y_ts_predict = scaler[i].inverse_transform(model.predict(x_ts_s[:,:,i], verbose=0))\n",
    "    \n",
    "        ts_predict = pd.DataFrame(y_ts_predict[:, 0], index=ts[:-1].index)\n",
    "    \n",
    "        mae = MeanAbsoluteError()\n",
    "        mape = MeanAbsolutePercentageError()\n",
    "    \n",
    "        mae.update_state(y_true[:, :, i], y_ts_predict)\n",
    "        mape.update_state(y_true[:, :, i], y_ts_predict)\n",
    "\n",
    "        i2, b2 = y_ts_predict.shape\n",
    "\n",
    "    # print(f'VARIABLE ==> [{col}]\\n')\n",
    "    # print(f'RMSE -> {round(root_mean_squared_error(y_true[:, :, i], y_ts_predict).numpy(), 3)} \\n')\n",
    "    # print(f'MAE ->  {round(mae.result().numpy(), 3)} \\n')\n",
    "    # print(f'MAPE -> { round(mape.result().numpy(), 3)} \\n')\n",
    "    # print(f'SW statistic: -> {shapiro(abs(y_ts_predict.reshape(i2, b2))).statistic} \\n')\n",
    "    # print(f'TRAIN TIME -> {round(finish, 3)} \\n')\n",
    "\n",
    "\n",
    "        with open(f'{data_path}/BiGru/metrics_result/{jp}_bi-gru_Iteration_{index}_results_ts.txt', 'a') as file:\n",
    "          file.write(f'VARIABLE ; [{col}]\\n')\n",
    "          file.write(f'RMSE ; {round(root_mean_squared_error(y_true[:, :, i], y_ts_predict).numpy(), 3)} \\n')\n",
    "          file.write(f'MAE ;  {round(mae.result().numpy(), 3)} \\n')\n",
    "          file.write(f'MAPE ; { round(mape.result().numpy(), 3)} \\n')\n",
    "    \n",
    "          #result = (abs(y_ts_predict.reshape(i2, b2)))\n",
    "          #file.write(f'SW statistic: -> {result.statistic}, p-value: {result.pvalue} \\n')\n",
    "    \n",
    "          file.write(f'SW statistic: ; {shapiro(abs(y_ts_predict.reshape(i2, b2))).statistic} \\n')\n",
    "          file.write(f'TRAIN TIME ; {round(finish, 3)} \\n')\n",
    "            \n",
    "\n",
    "        plt.figure(figsize=(24, 10))\n",
    "        plt.subplot(1, 1, 1)\n",
    "        plt.plot(ts.index, ts[[col]].values,  label='Real', linewidth=line_width)\n",
    "        plt.plot(ts_predict.index, ts_predict.values, label='Predicted BiGRU', linewidth=line_width)\n",
    "        plt.title(f'Iteration {index} - {col}', fontdict={'fontsize': 30})\n",
    "        plt.xlabel('Time', fontsize=fsize_labels*2)\n",
    "        plt.ylabel('Value', fontsize=fsize_labels*2)\n",
    "\n",
    "        plt.xticks(fontsize=font_size_ticks*2)\n",
    "        plt.yticks(fontsize=font_size_ticks*2)\n",
    "        plt.tight_layout()\n",
    "    \n",
    "        plt.grid(False)\n",
    "        plt.legend(fontsize=25)\n",
    "        plt.savefig(f'{data_path}/BiGru/test_result/iteration_{index}_{col.replace(\"/\", \"_\")}_test_result.png')\n",
    "        plt.show()\n",
    "\n",
    "        fig, axe = plt.subplots(figsize=(5, 7))\n",
    "        box = axe.boxplot(ts_predict.values.shape - ts[[col]].values[:-1], notch=True, vert=True, autorange=True, sym=\"\")\n",
    "        for box in box['boxes']:\n",
    "          box.set_color('b')\n",
    "          axe.set_title(f'Iteration {index} - {col}', fontsize=22)\n",
    "          axe.tick_params(labelsize=20)\n",
    "          plt.tight_layout()\n",
    "          plt.savefig(f'{data_path}/BiGru/test_result/iteration-{index}_{cut_col_name(col)}_test_result_box_plot.png')\n",
    "        # break\n",
    "      # break\n",
    "      index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYmEzZiI_Kqc"
   },
   "source": [
    "# 3. LSTM Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1722828996767,
     "user": {
      "displayName": "Ximena Guerron",
      "userId": "14256164877379905553"
     },
     "user_tz": 300
    },
    "id": "8EV28Iwdt9Xc"
   },
   "outputs": [],
   "source": [
    "# Loss: the RMSE (root mean squared error) will be used for training because it allows to have errors in the same units as each variable.\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    rmse = tf.math.sqrt(tf.math.reduce_mean(tf.square(y_pred-y_true)))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1722828996767,
     "user": {
      "displayName": "Ximena Guerron",
      "userId": "14256164877379905553"
     },
     "user_tz": 300
    },
    "id": "YmK8SOgL9340"
   },
   "outputs": [],
   "source": [
    "dict_variables_results = {}\n",
    "\n",
    "for i, col in enumerate(df.columns):\n",
    "  dict_variables_results[col] = {\n",
    "      \"rmse\": 0,\n",
    "      \"mae\": 0,\n",
    "      \"mape\": 0,\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 240
    },
    "executionInfo": {
     "elapsed": 15152,
     "status": "error",
     "timestamp": 1722847260780,
     "user": {
      "displayName": "Ximena Guerron",
      "userId": "14256164877379905553"
     },
     "user_tz": 300
    },
    "id": "DpynICJvX_B5",
    "outputId": "428b6f8a-1ad2-43f8-e885-47b5879472b1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model creation\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Bidirectional, Flatten, Conv1D, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam, Adagrad\n",
    "from keras.regularizers import l2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from scipy.stats import shapiro\n",
    "from time import time\n",
    "\n",
    "# Loss: the RMSE (root mean squared error) will be used for training because it allows to have errors in the same units as each variable.\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    rmse = tf.math.sqrt(tf.math.reduce_mean(tf.square(y_pred-y_true)))\n",
    "    return rmse\n",
    "\n",
    "for jp in range(10):\n",
    "\n",
    "  index = 0\n",
    "\n",
    "  print('RUN N°', jp)\n",
    "\n",
    "  for (x_tr_s, y_tr_s), (x_vl_s, y_vl_s), (x_ts_s, y_ts_s) in zip(tr_s_ds.values(), vl_s_ds.values(), ts_s_ds.values()):\n",
    "\n",
    "    print('--' * 20)\n",
    "    print(f'Iteration {index}: {x_tr_s.shape}, {x_ts_s.shape}')\n",
    "    print('--' * 20)\n",
    "\n",
    "    scaler = scalers[index]\n",
    "    tr_cut_date, vl_cut_date, ts_cut_date = cut_dates[index]\n",
    "\n",
    "    tr, vl, ts = datasets[index]\n",
    "    x_ts, y_true = ts_datasets[index]\n",
    "\n",
    "    for i, col in enumerate(df.columns):\n",
    "\n",
    "      start = time()\n",
    "\n",
    "      print(f'Training LSTM... for {col}')\n",
    "\n",
    "      model = Sequential([\n",
    "          LSTM(128, input_shape=(x_tr_ds.shape[1], 1), return_sequences = True,),\n",
    "          Dropout(0.1),\n",
    "          LSTM(128, return_sequences = True),\n",
    "          LSTM(64, return_sequences = False),\n",
    "          Dense(16, activation='relu'),\n",
    "          Dense(1, activation='linear')]\n",
    "      )\n",
    "\n",
    "\n",
    "      model.compile(optimizer = RMSprop(5e-5), loss = root_mean_squared_error, metrics = [MeanAbsoluteError(), MeanAbsolutePercentageError()])\n",
    "\n",
    "      # Training (approximately 1 min using GPU)\n",
    "\n",
    "      EPOCHS = 250\n",
    "      BATCH_SIZE = 256\n",
    "\n",
    "      story = model.fit(\n",
    "          x = np.expand_dims(x_tr_s[:,:,i], axis=-1),\n",
    "          y = np.expand_dims(y_tr_s[:,:,i], axis=-1),\n",
    "          batch_size = BATCH_SIZE,\n",
    "          epochs = EPOCHS,\n",
    "          validation_data = (np.expand_dims(x_vl_s[:,:,i], axis=-1), np.expand_dims(y_vl_s[:,:,i], axis=-1)),\n",
    "          verbose=2,\n",
    "          callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)],\n",
    "      )\n",
    "\n",
    "      finish = time() - start\n",
    "\n",
    "      plt.figure(figsize=(15, 7))\n",
    "      plt.subplot(1, 1, 1)\n",
    "      plt.plot(story.history['loss'], label='RMSE train', linewidth=line_width)\n",
    "      plt.plot(story.history['val_loss'],label='RMSE val', linewidth=line_width)\n",
    "      plt.title(f'Iteration {index} - {col}', fontsize=28)\n",
    "      plt.xlabel('Epoch', fontsize=20)\n",
    "      plt.ylabel('RMSE', fontsize=20)\n",
    "      plt.xticks(fontsize=font_size_ticks)\n",
    "      plt.yticks(fontsize=font_size_ticks)\n",
    "      plt.tight_layout()\n",
    "      plt.legend(fontsize=22);\n",
    "\n",
    "      plt.grid(False)\n",
    "      plt.savefig(f'{data_path}/LSTM/metrics_result/iteration_{index}_{col.replace(\"/\", \"_\")}_rmse.png')\n",
    "      plt.tight_layout()\n",
    "      plt.show()\n",
    "\n",
    "      plt.figure(figsize=(15, 7))\n",
    "      plt.subplot(1, 1, 1)\n",
    "      plt.plot(story.history['mean_absolute_error'], label='MAE train', linewidth=line_width)\n",
    "      plt.plot(story.history['val_mean_absolute_error'],label='MAE val', linewidth=line_width)\n",
    "      plt.title(f'Iteration {index} - {col}',  fontsize=28)\n",
    "      plt.xlabel('Epoch', fontsize=20)\n",
    "      plt.ylabel('MAE', fontsize=20)\n",
    "      plt.xticks(fontsize=font_size_ticks)\n",
    "      plt.yticks(fontsize=font_size_ticks)\n",
    "      plt.tight_layout()\n",
    "      plt.legend(fontsize=22);\n",
    "\n",
    "      plt.grid(False)\n",
    "      plt.savefig(f'{data_path}/LSTM/metrics_result/iteration_{index}_{col.replace(\"/\", \"_\")}_mae.png')\n",
    "      plt.tight_layout()\n",
    "      plt.show()\n",
    "\n",
    "      plt.figure(figsize=(15, 7))\n",
    "      plt.subplot(1, 1, 1)\n",
    "      plt.plot(story.history['mean_absolute_percentage_error'], label='MAPE train', linewidth=line_width)\n",
    "      plt.plot(story.history['val_mean_absolute_percentage_error'],label='MAPE val', linewidth=line_width)\n",
    "      plt.title(f'Iteration {index} - {col}', fontsize=28)\n",
    "      plt.xlabel('Epoch', fontsize=20)\n",
    "      plt.ylabel('MAPE', fontsize=20)\n",
    "      plt.xticks(fontsize=font_size_ticks)\n",
    "      plt.yticks(fontsize=font_size_ticks)\n",
    "      plt.tight_layout()\n",
    "      plt.legend(fontsize=22);\n",
    "\n",
    "      plt.grid(False)\n",
    "      plt.savefig(f'{data_path}/LSTM/metrics_result/iteration_{index}_{col.replace(\"/\", \"_\")}_mape.png')\n",
    "      plt.tight_layout()\n",
    "      plt.show()\n",
    "\n",
    "      story = None\n",
    "\n",
    "      y_ts_predict = scaler[i].inverse_transform(model.predict(x_ts_s[:,:,i], verbose=0))\n",
    "\n",
    "      ts_predict = pd.DataFrame(y_ts_predict[:, 0], index=ts[:-1].index)\n",
    "\n",
    "      mae = MeanAbsoluteError()\n",
    "      mape = MeanAbsolutePercentageError()\n",
    "\n",
    "      mae.update_state(y_true[:, :, i], ts_predict.values)\n",
    "      mape.update_state(y_true[:, :, i], ts_predict.values)\n",
    "\n",
    "      i2, b2 = y_ts_predict.shape\n",
    "\n",
    "      # with open(f'./lstm-fold_{index}_results_ts.txt', 'a') as file:\n",
    "      with open(f'{data_path}/LSTM/metrics_result/{jp}_Iteration_{index}_results_ts.txt', 'a') as file:\n",
    "        file.write(f'VARIABLE ==> [{col}]\\n')\n",
    "        file.write(f'RMSE -> {round(root_mean_squared_error(y_true[:, :, i], ts_predict.values).numpy(), 3)} \\n')\n",
    "        file.write(f'MAE ->  {round(mae.result().numpy(), 3)} \\n')\n",
    "        file.write(f'MAPE -> { round(mape.result().numpy(), 3)} \\n')\n",
    "        file.write(f'SW statistic: -> {shapiro(abs(y_ts_predict.reshape(i2, b2))).statistic} \\n')\n",
    "        file.write(f'TRAIN TIME -> {round(finish, 3)} \\n')\n",
    "\n",
    "      plt.figure(figsize=(24, 10))\n",
    "      plt.subplot(1, 1, 1)\n",
    "      plt.plot(ts.index, ts[[col]].values, label='Real', linewidth=line_width)\n",
    "      plt.plot(ts_predict.index, ts_predict.values, label='Predicted LSTM', linewidth=line_width)\n",
    "      plt.title(f'Iteration {index} - {col}', fontdict={'fontsize': 28})\n",
    "      plt.xlabel('Time', fontsize=20)\n",
    "      plt.ylabel('Value', fontsize=20)\n",
    "\n",
    "      plt.xticks(fontsize=font_size_ticks)\n",
    "      plt.yticks(fontsize=font_size_ticks)\n",
    "      plt.grid(False)\n",
    "      plt.legend(fontsize=22)\n",
    "      plt.savefig(f'{data_path}/LSTM/test_result/iteration_{index}_{col.replace(\"/\", \"_\")}_test_result.png')\n",
    "      plt.show()\n",
    "\n",
    "      fig, axe = plt.subplots(figsize=(5, 7))\n",
    "      box = axe.boxplot(ts_predict.values - ts[[col]][:-1].values, notch=True, vert=True, autorange=True, sym=\"\")\n",
    "      for box in box['boxes']:\n",
    "        box.set_color('b')\n",
    "        axe.set_title(f'Iteration {index} - {col}', fontsize=23)\n",
    "        axe.tick_params(labelsize=23)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{data_path}/LSTM/test_result/iteration-{index}_{cut_col_name(col)}_test_result_box_plot.jpg')\n",
    "      # break\n",
    "\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5XhZLMMMTirJ"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4z1z1NrY_IYl"
   },
   "source": [
    "# 3. ARIMA Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10769,
     "status": "ok",
     "timestamp": 1722829007526,
     "user": {
      "displayName": "Ximena Guerron",
      "userId": "14256164877379905553"
     },
     "user_tz": 300
    },
    "id": "Nk2enkq9DSpn",
    "outputId": "1ee1f3be-3b35-43c6-c023-85cbd1da2fca"
   },
   "outputs": [],
   "source": [
    "!pip install pmdarima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1348,
     "status": "ok",
     "timestamp": 1722829014145,
     "user": {
      "displayName": "Ximena Guerron",
      "userId": "14256164877379905553"
     },
     "user_tz": 300
    },
    "id": "i_KVUj9mcug5"
   },
   "outputs": [],
   "source": [
    "def metrics_values(y_true, y_pred):\n",
    "\n",
    "  for index, column in enumerate(tr.columns):\n",
    "    mae = MeanAbsoluteError()\n",
    "    mape = MeanAbsolutePercentageError()\n",
    "    mae.update_state(y_true[:, :, index], y_pred[:, :, index])\n",
    "    mape.update_state(y_true[:, :, index], y_pred[:, :, index])\n",
    "\n",
    "    with open('results_tr.txt', 'a') as file:\n",
    "      file.write(f'VARIABLE;[{column}]\\n')\n",
    "      file.write(f'RMSE;{round(root_mean_squared_error(y_true[:, :, index], y_pred[:, :, index]).numpy(), 3)} \\n')\n",
    "      file.write(f'MAE;{round(mae.result().numpy(), 3)} \\n')\n",
    "      file.write(f'MAPE;{ round(mape.result().numpy(), 3)} \\n')\n",
    "    # file.write(f'MASE -> { round(mase.result().numpy(), 3)} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4723,
     "status": "ok",
     "timestamp": 1722829027997,
     "user": {
      "displayName": "Ximena Guerron",
      "userId": "14256164877379905553"
     },
     "user_tz": 300
    },
    "id": "cVnXbF4f0V7r"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import RootMeanSquaredError, MeanAbsoluteError, MeanAbsolutePercentageError, MeanSquaredError\n",
    "from pmdarima import ARIMA, auto_arima\n",
    "from scipy.stats import shapiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1722829030296,
     "user": {
      "displayName": "Ximena Guerron",
      "userId": "14256164877379905553"
     },
     "user_tz": 300
    },
    "id": "Q_a9tN7my1OL"
   },
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    rmse = tf.math.sqrt(tf.math.reduce_mean(tf.square(y_pred-y_true)))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1S4Zr0Y_-XDTotzjsgrMlX_iwvQPDnmvu"
    },
    "executionInfo": {
     "elapsed": 915355,
     "status": "ok",
     "timestamp": 1722830388124,
     "user": {
      "displayName": "Ximena Guerron",
      "userId": "14256164877379905553"
     },
     "user_tz": 300
    },
    "id": "q61z-lbLkxA5",
    "outputId": "eb288a97-b97a-4841-c4f1-a07ea86cbf48"
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "#index = 0\n",
    "print(\"ARIMA\")\n",
    "\n",
    "#dict_variables = {}\n",
    "\n",
    "#dict_variables_results = {}\n",
    "\n",
    "#for i, col in enumerate(df.columns):\n",
    "#  dict_variables_results[col] = {\n",
    "#      \"rmse\": 0,\n",
    "#      \"mae\": 0,\n",
    "#      \"mape\": 0,\n",
    "#  }\n",
    "\n",
    "font_size_ticks = 20\n",
    "\n",
    "for jp in range(10):\n",
    "  \n",
    "    index = 0\n",
    "    \n",
    "    print('RUN N°', jp)\n",
    "    \n",
    "    for (x_tr_s, y_tr_s), (x_vl_s, y_vl_s), (x_ts_s, y_ts_s) in zip(tr_s_ds.values(), vl_s_ds.values(), ts_s_ds.values()):\n",
    "    \n",
    "        print('--' * 20)\n",
    "        print(f'Training ARIMA... for {col} {x_tr_s[:,:,i].shape} ---- {tr.index[:-1].shape}')\n",
    "        print('--' * 20)\n",
    "    \n",
    "        scaler = scalers[index]\n",
    "        tr_cut_date, vl_cut_date, ts_cut_date = cut_dates[index]\n",
    "    \n",
    "        tr, vl, ts = datasets[index]\n",
    "        x_ts, y_true = ts_datasets[index]\n",
    "    \n",
    "        for i, col in enumerate(df.columns):\n",
    "          start = time()\n",
    "    \n",
    "          print(f'Training ARIMA... for {col}')\n",
    "    \n",
    "          #modelo.summary()\n",
    "    \n",
    "          modelo = ARIMA(order=(1, 1, 1), seasonal_order=(1, 1, 1, 7))\n",
    "          modelo.fit(y=pd.DataFrame(x_tr_s[:,:,i], index=tr.index[:-1])) # tomo hasta la penultima fecha\n",
    "    \n",
    "          #predicciones_pdmarima = modelo.predict(len(x_vl_s.shape[0]) + len(x_ts_s.shape[0]))\n",
    "    \n",
    "          predicciones_pdmarima = modelo.predict(len(vl) + len(ts))\n",
    "          predicciones_pdmarima.name = 'predicciones_pdmarima'\n",
    "          pred_arima = pd.DataFrame(predicciones_pdmarima[predicciones_pdmarima.index > str(vl_cut_date)])\n",
    "    \n",
    "          plt.figure(figsize=(18, 15))\n",
    "          plt.subplot(2, 1, 1)\n",
    "    \n",
    "          scaled_predicted = scaler[i].inverse_transform(pred_arima.values)\n",
    "    \n",
    "          finish = time() - start\n",
    "    \n",
    "          # pd.DataFrame({'predict': scaler[i].inverse_transform(pred_arima.values).squeeze(), 'real': ts[[col]].values.squeeze()}, index=ts.index).to_csv('result')\n",
    "    \n",
    "          plt.plot(ts.index[:-1], ts[[col]][:-1], label='Real', linewidth=line_width)\n",
    "          plt.plot(pred_arima.index, scaled_predicted, label='Estimated ARIMA - pmdarima', linewidth=line_width)\n",
    "          plt.title(f'Iteration {index} - {col}', fontdict={'fontsize': 28})\n",
    "          plt.legend(fontsize=25)\n",
    "          plt.xlabel('Time', fontsize=30)\n",
    "          plt.ylabel('Value', fontsize=30)\n",
    "    \n",
    "          plt.xticks(fontsize=font_size_ticks) # aumento de datos\n",
    "          plt.yticks(fontsize=font_size_ticks)\n",
    "    \n",
    "          plt.grid(False)\n",
    "          plt.savefig(f'{data_path}/Arima/test_result/Iteration-{index}_{cut_col_name(col)}_test_result.png')\n",
    "          plt.tight_layout()\n",
    "          plt.show()\n",
    "    \n",
    "    #      story = None\n",
    "            \n",
    "          mae = MeanAbsoluteError()\n",
    "          mape = MeanAbsolutePercentageError()\n",
    "    \n",
    "          mae.update_state(y_true[:, :, i], scaled_predicted)\n",
    "          mape.update_state(y_true[:, :, i], scaled_predicted)\n",
    "    \n",
    "          i2, b2 = scaled_predicted.shape\n",
    "    \n",
    "          with open(f'{data_path}/Arima/metrics_result/{jp}_Arima_Iteration_{index}_results_ts.txt', 'a') as file:\n",
    "          # with open(f'./AutoArima-fold_{index}_results_ts.txt', 'a') as file:\n",
    "            file.write(f'VARIABLE;[{col}]\\n')\n",
    "            file.write(f'RMSE;{round(root_mean_squared_error(y_true[:, :, i], scaled_predicted).numpy(), 3)} \\n')\n",
    "            file.write(f'MAE;{round(mae.result().numpy(), 3)} \\n')\n",
    "            file.write(f'MAPE;{ round(mape.result().numpy(), 3)} \\n')\n",
    "            file.write(f'SW statistic;{shapiro(abs(scaled_predicted.reshape(i2, b2))).statistic} \\n')\n",
    "            file.write(f'TRAIN TIME;{round(finish, 3)} \\n')\n",
    "    \n",
    "          dict_variables_results[col]['rmse'] = dict_variables_results[col]['rmse'] + round(root_mean_squared_error(y_true[:, :, i], scaled_predicted).numpy(), 3)\n",
    "          dict_variables_results[col]['mae'] = dict_variables_results[col]['mae'] + round(mae.result().numpy(), 3)\n",
    "          dict_variables_results[col]['mape'] = dict_variables_results[col]['mape'] + round(mape.result().numpy(), 3)\n",
    "    \n",
    "          fig, axe = plt.subplots(figsize=(5, 7))\n",
    "          box = axe.boxplot(scaled_predicted - ts[[col]][:-1], notch=True, vert=True, autorange=True, sym=\"\")\n",
    "          for box in box['boxes']:\n",
    "            box.set_color('b')\n",
    "            axe.set_title(f'Iteration {index} - {col}', fontsize=23)\n",
    "            axe.tick_params(labelsize=23)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'{data_path}/Arima/test_result/Iteration-{index}_{cut_col_name(col)}_test_result_box_plot.png')\n",
    "    \n",
    "        \n",
    "        plt.show()\n",
    "        # break\n",
    "    \n",
    "    \n",
    "        for col in df.columns:\n",
    "            with open(f'{data_path}/Arima/metrics_result/{jp}_Iterations_results_summary.txt', 'a') as file:\n",
    "              file.write(f'VARIABLE; [{col}]\\n')\n",
    "              file.write(f'RMSE; {dict_variables_results[col][\"rmse\"]} \\n')\n",
    "              file.write(f'MAE; {dict_variables_results[col][\"mae\"]} \\n')\n",
    "              file.write(f'MAPE; {dict_variables_results[col][\"mape\"]} \\n')\n",
    "    \n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWw9sf-Qkywb"
   },
   "source": [
    "# 4. AUTO ARIMA Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1OM4Sr37Q5h7fwuxfQ3fGt4ddpBVMoB_p"
    },
    "executionInfo": {
     "elapsed": 996245,
     "status": "error",
     "timestamp": 1722847043620,
     "user": {
      "displayName": "Ximena Guerron",
      "userId": "14256164877379905553"
     },
     "user_tz": 300
    },
    "id": "plUUL-2DHZ3o",
    "outputId": "4f1069c6-896c-4138-80ca-61aa2afb8cb5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from time import time\n",
    "\n",
    "index = 0\n",
    "\n",
    "dict_variables = {}\n",
    "\n",
    "dict_variables_results = {}\n",
    "\n",
    "for i, col in enumerate(df.columns):\n",
    "  dict_variables_results[col] = {\n",
    "      \"rmse\": 0,\n",
    "      \"mae\": 0,\n",
    "      \"mape\": 0,\n",
    "      \"time\": 0,\n",
    "  }\n",
    "\n",
    "\n",
    "for jp in range(1):\n",
    "\n",
    "  for (x_tr_s, y_tr_s), (x_vl_s, y_vl_s), (x_ts_s, y_ts_s) in zip(tr_s_ds.values(), vl_s_ds.values(), ts_s_ds.values()):\n",
    "\n",
    "    print('--' * 20)\n",
    "    print(f'Training ARIMA... for {col} {x_tr_s[:,:,i].shape} ---- {tr.index[:-1].shape}')\n",
    "    print('--' * 20)\n",
    "\n",
    "    scaler = scalers[index]\n",
    "    tr_cut_date, vl_cut_date, ts_cut_date = cut_dates[index]\n",
    "\n",
    "    tr, vl, ts = datasets[index]\n",
    "    x_ts, y_true = ts_datasets[index]\n",
    "\n",
    "    for i, col in enumerate(df.columns):\n",
    "      start = time()\n",
    "\n",
    "      print(f'Training AUTOARIMA... for {col}')\n",
    "\n",
    "      modelo = auto_arima(pd.DataFrame(x_tr_s[:,:,i], index=tr.index[:-1]),\n",
    "                        m=12,               # frequency of series\n",
    "                        d=None,             # let model determine 'd'\n",
    "                        test='adf',         # use adftest to find optimal 'd'\n",
    "                        start_p=0, start_q=0, # minimum p and q\n",
    "                        max_p=12, max_q=12, # maximum p and q\n",
    "                        D=None,             # let model determine 'D'\n",
    "                        trace=True,\n",
    "                        error_action='ignore',\n",
    "                        suppress_warnings=True,\n",
    "                        stepwise=True,\n",
    "      )\n",
    "\n",
    "      modelo.summary()\n",
    "\n",
    "\n",
    "      predicciones_pdmarima = modelo.predict(len(vl) + len(ts))\n",
    "      predicciones_pdmarima.name = 'predicciones_pdmarima'\n",
    "      pred_arima = pd.DataFrame(predicciones_pdmarima[predicciones_pdmarima.index > str(vl_cut_date)])\n",
    "\n",
    "      plt.figure(figsize=(18, 15))\n",
    "      plt.subplot(2, 1, 1)\n",
    "\n",
    "      scaled_predicted = scaler[i].inverse_transform(pred_arima.values)\n",
    "\n",
    "      finish = time() - start\n",
    "\n",
    "      # pd.DataFrame({'predict': scaler[i].inverse_transform(pred_arima.values).squeeze(), 'real': ts[[col]].values.squeeze()}, index=ts.index).to_csv('result')\n",
    "\n",
    "      plt.plot(ts.index[:-1], ts[[col]][:-1], label='Real', linewidth=line_width)\n",
    "      plt.plot(pred_arima.index, scaled_predicted, label='Predicted ARIMA - pmdarima', linewidth=line_width)\n",
    "      plt.title(f'Iteration {index} - {col}', fontdict={'fontsize': 28})\n",
    "      plt.legend(fontsize=22)\n",
    "\n",
    "      plt.xticks(fontsize=20) # aumento de datos\n",
    "      plt.yticks(fontsize=20)\n",
    "\n",
    "      plt.grid(False)\n",
    "      plt.savefig(f'{data_path}/AutoArima/test_result/Iteration-{index}_{cut_col_name(col)}_test_result.jpg')\n",
    "      plt.tight_layout()\n",
    "      plt.show()\n",
    "\n",
    "      mae = MeanAbsoluteError()\n",
    "      mape = MeanAbsolutePercentageError()\n",
    "\n",
    "      mae.update_state(y_true[:, :, i], scaled_predicted)\n",
    "      mape.update_state(y_true[:, :, i], scaled_predicted)\n",
    "\n",
    "      i2, b2 = scaled_predicted.shape\n",
    "\n",
    "      with open(f'{data_path}/AutoArima/metrics_result/{jp}_Iteration_{index}_results_ts.txt', 'a') as file:\n",
    "      # with open(f'./AutoArima-fold_{index}_results_ts.txt', 'a') as file:\n",
    "        file.write(f'VARIABLE ==> [{col}]\\n')\n",
    "        file.write(f'RMSE -> {round(root_mean_squared_error(y_true[:, :, i], scaled_predicted).numpy(), 3)} \\n')\n",
    "        file.write(f'MAE ->  {round(mae.result().numpy(), 3)} \\n')\n",
    "        file.write(f'MAPE -> { round(mape.result().numpy(), 3)} \\n')\n",
    "        file.write(f'SW statistic: -> {shapiro(abs(scaled_predicted.reshape(i2, b2))).statistic} \\n')\n",
    "        file.write(f'TRAIN TIME -> {round(finish, 3)} \\n')\n",
    "\n",
    "      dict_variables_results[col]['rmse'] = dict_variables_results[col]['rmse'] + round(root_mean_squared_error(y_true[:, :, i], scaled_predicted).numpy(), 3)\n",
    "      dict_variables_results[col]['mae'] = dict_variables_results[col]['mae'] + round(mae.result().numpy(), 3)\n",
    "      dict_variables_results[col]['mape'] = dict_variables_results[col]['mape'] + round(mape.result().numpy(), 3)\n",
    "      dict_variables_results[col]['time'] = dict_variables_results[col]['time'] + round(finish, 3)\n",
    "\n",
    "      fig, axe = plt.subplots(figsize=(5, 7))\n",
    "      box = axe.boxplot(scaled_predicted - ts[[col]][:-1], notch=True, vert=True, autorange=True, sym=\"\")\n",
    "      for box in box['boxes']:\n",
    "        box.set_color('b')\n",
    "        axe.set_title(f'Iteration {index} - {col}', fontsize=23)\n",
    "        axe.tick_params(labelsize=23)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{data_path}/AutoArima/test_result/Iteration-{index}_{cut_col_name(col)}_test_result_box_plot.jpg')\n",
    "\n",
    "    index += 1\n",
    "\n",
    "    plt.show()\n",
    "    # break\n",
    "\n",
    "\n",
    "  for col in df.columns:\n",
    "    with open(f'{data_path}/AutoArima/metrics_result/{jp}_Iterations_results_summary.txt', 'a') as file:\n",
    "      file.write(f'VARIABLE , [{col}]\\n')\n",
    "      file.write(f'RMSE , {dict_variables_results[col][\"rmse\"]} \\n')\n",
    "      file.write(f'MAE , {dict_variables_results[col][\"mae\"]} \\n')\n",
    "      file.write(f'MAPE , {dict_variables_results[col][\"mape\"]} \\n')\n",
    "      file.write(f'TRAIN TIME , {dict_variables_results[col][\"time\"]} \\n')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "executionInfo": {
     "elapsed": 4815,
     "status": "error",
     "timestamp": 1722828302679,
     "user": {
      "displayName": "Ximena Guerron",
      "userId": "14256164877379905553"
     },
     "user_tz": 300
    },
    "id": "AkgqA4qF8fV9",
    "outputId": "e940bfe3-a613-4c1f-e7a7-fc4f3ca43de3"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1GF2WekDAEqIdHwUHBFWwluLvcWSzccC6",
     "timestamp": 1722828711061
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
